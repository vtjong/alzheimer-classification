{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "%matplotlib inline\n",
    "import os, sys, re\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn\n",
    "\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "import argparse\n",
    "\"\"\" Training and hyperparameter search configurations \"\"\"\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Alzheimer Classification Tester')\n",
    "parser.add_argument('--oasis2_path', type=str, default='/Users/valenetjong/Downloads/OAS2_RAW_PART1',\n",
    "                    help='directory to oasis 2 download')\n",
    "parser.add_argument('--img_dir', type=str, default='/Users/valenetjong/alzheimer-classification/oasis2',\n",
    "                    help='directory for image storage')\n",
    "parser.add_argument('--oasis2csv_path', type=str, default='/Users/valenetjong/alzheimer-classification/datacsv/oasis_longitudinal.csv',\n",
    "                    help='path to oasis 2 csv')\n",
    "parser.add_argument('--process_flag', type=bool, default=False,\n",
    "                    help=\"extract files from disk if True, use already extracted files, if False\")\n",
    "parser.add_argument('--create_dataset', type=bool, default=True,\n",
    "                    help=\"create dataset from scratch if True, load in processed dataset if False\")\n",
    "parser.add_argument('--best_custom_model_path', type=str, default='/Users/valenetjong/alzheimer-classification/models/DeepCNNModel_epoch33.pt',\n",
    "                    help='path to best custom model for testing')\n",
    "parser.add_argument('--best_resnet_model_path', type=str, default='/Users/valenetjong/alzheimer-classification/models/ResNet.pt',\n",
    "                    help='path to best resnet model for testing')\n",
    "parser.add_argument('--num_classes', type=int, default=3,\n",
    "                    help='number of classes')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed (default: 1)')\n",
    "args = parser.parse_args('')\n",
    "# Set random seed to reproduce results\n",
    "torch.manual_seed(args.seed)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1275292b0>"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading and Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "DEMENTIA_MAP = {\n",
    "    '0.0': \"nondemented\",\n",
    "    '0.5': \"mildly demented\",\n",
    "    '1.0': 'moderately demented',\n",
    "}\n",
    "\n",
    "def convert_and_rename_hdr_img_to_nifti(base_dir, output_dir, oasis2_csv_path, slice_idx=140):\n",
    "    \"\"\"\n",
    "    Convert and rename .hdr/.img files to .nifti format.\n",
    "\n",
    "    Parameters:\n",
    "    base_dir (str): Base directory containing the subdirectories.\n",
    "    output_dir (str): Directory where .nifti files will be saved.\n",
    "    oasis2_csv_path: Path to the CSV file containing Oasis 2 metadata.\n",
    "    \"\"\"\n",
    "    oasis_df = pd.read_csv(oasis2_csv_path)\n",
    "\n",
    "    # Check if the output directory exists, if not, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Search for the specific subdirectories\n",
    "    subdirectories = glob.glob(os.path.join(base_dir, 'OAS2_*_MR1'))\n",
    "\n",
    "    for sub_dir in subdirectories:\n",
    "        # Construct the path to the RAW folder\n",
    "        raw_dir = os.path.join(sub_dir, 'RAW')\n",
    "\n",
    "        # Check if RAW directory exists\n",
    "        if os.path.exists(raw_dir):\n",
    "            # Construct the file path for hdr file\n",
    "            hdr_file = os.path.join(raw_dir, 'mpr-2.nifti.hdr')\n",
    "            pid = os.path.basename(sub_dir)\n",
    "            row = oasis_df.loc[oasis_df['MRI ID'] == pid]\n",
    "            dementia_type = str(row['CDR'].item())\n",
    "\n",
    "            # Check if hdr file exists\n",
    "            if os.path.exists(hdr_file):\n",
    "                # Load the image (this should automatically include the associated .img file)\n",
    "                img = nib.load(hdr_file)\n",
    "                data = img.get_fdata()\n",
    "                if len(data.shape) == 4:\n",
    "                    data = data[:, :, :, 0]\n",
    "                slice_2d = data[:, slice_idx, :]\n",
    "                normalized_slice = (slice_2d - np.min(slice_2d)) / (np.max(slice_2d) - np.min(slice_2d))\n",
    "                scaled_slice = (255 * normalized_slice).astype(np.uint8)\n",
    "\n",
    "                # Save the slice as a PNG\n",
    "                image = Image.fromarray(scaled_slice)\n",
    "                png_file_name = os.path.basename(hdr_file).replace('.nifti.hdr', '.png')\n",
    "                processed_dir = os.path.join(output_dir, DEMENTIA_MAP[dementia_type])\n",
    "                os.makedirs(processed_dir, exist_ok=True)\n",
    "                png_file_path = os.path.join(processed_dir, f'{pid}_{png_file_name}')\n",
    "                image.save(png_file_path)\n",
    "                print(f'Processed and saved {png_file_path}')\n",
    "            else:\n",
    "                print(f'No .hdr file found in {raw_dir}')\n",
    "        else:\n",
    "            print(f'No RAW directory found in {sub_dir}')\n",
    "\n",
    "if args.process_flag:\n",
    "    in_dir = args.oasis2_path\n",
    "    out_dir = os.path.join(args.img_dir, 'raw')\n",
    "    oasis2_csv = args.oasis2csv_path\n",
    "    convert_and_rename_hdr_img_to_nifti(in_dir, out_dir, oasis2_csv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "import cv2 as cv\n",
    "import tempfile\n",
    "import shutil\n",
    "import skimage.exposure\n",
    "\n",
    "\"\"\" Pre-processing Functions \"\"\"\n",
    "# Pre-determined max dimensions of cropped images\n",
    "CONV_WIDTH = 137\n",
    "CONV_HEIGHT = 167\n",
    "\n",
    "def convert_to_grayscale(img):\n",
    "    \"\"\"\n",
    "    Converts an image to grayscale. Handles images with alpha channel.\n",
    "    \"\"\"\n",
    "    if img.mode in [\"RGBA\", \"LA\"] or (img.mode == \"P\" and 'transparency' in img.info):\n",
    "        # Use alpha channel as mask\n",
    "        alpha = img.split()[-1]\n",
    "        bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        bg.paste(img, mask=alpha)\n",
    "        return np.array(bg.convert('L'))\n",
    "    else:\n",
    "        return np.array(img.convert('L'))\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    \"\"\"\n",
    "    Normalizes the intensity of an image to the range [0, 255].\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    Normalized image.\n",
    "    \"\"\"\n",
    "    img_min = img.min()\n",
    "    img_max = img.max()\n",
    "    normalized_img = (img - img_min) / (img_max - img_min) * 255\n",
    "    return normalized_img.astype(np.uint8)\n",
    "\n",
    "def apply_low_pass_filter(img, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Applies a Gaussian blur (low-pass filter) to the image.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be filtered.\n",
    "    kernel_size: Size of the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "    Filtered image.\n",
    "    \"\"\"\n",
    "    return cv.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def increase_intensity(img):\n",
    "    \"\"\"\n",
    "    Increases the intensity of an image using contrast stretching.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image whose intensity is to be increased.\n",
    "\n",
    "    Returns:\n",
    "    Image with increased intensity.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(img, (2, 98))\n",
    "    return skimage.exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "\n",
    "def pad_image_to_size(img, width, height):\n",
    "    \"\"\"\n",
    "    Pads an image with zeros to the specified width and height.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be padded.\n",
    "    width: The desired width.\n",
    "    height: The desired height.\n",
    "\n",
    "    Returns:\n",
    "    Padded image.\n",
    "    \"\"\"\n",
    "    padded_img = np.zeros((height, width), dtype=img.dtype)\n",
    "    y_offset = (height - img.shape[0]) // 2\n",
    "    x_offset = (width - img.shape[1]) // 2\n",
    "    padded_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    return padded_img\n",
    "\n",
    "def crop_black_boundary(mri_image, kernel_size=50):\n",
    "    \"\"\"\n",
    "    Crops the black boundary from an MRI image, while ignoring small noise within the black regions.\n",
    "\n",
    "    Parameters:\n",
    "    mri_image: Input MRI image.\n",
    "\n",
    "    Returns:\n",
    "    Cropped MRI image with black boundaries removed.\n",
    "    \"\"\"\n",
    "    # Thresholding to get the binary image for contour detection\n",
    "    _, thresh = cv.threshold(mri_image, 1, 255, cv.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to remove small noise\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    cleaned = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Finding contours\n",
    "    contours, _ = cv.findContours(cleaned, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours found, return original image\n",
    "    if not contours:\n",
    "        return mri_image\n",
    "\n",
    "    largest_contour = max(contours, key=cv.contourArea)\n",
    "    x, y, w, h = cv.boundingRect(largest_contour)\n",
    "    cropped_image = mri_image[y:y+h, x:x+w]\n",
    "    return cropped_image\n",
    "\n",
    "def process_image(fn, target_subdir):\n",
    "    \"\"\"\n",
    "    Processes a single MRI image file.\n",
    "    \"\"\"\n",
    "    with Image.open(fn) as img:\n",
    "        img_gray = convert_to_grayscale(img)\n",
    "\n",
    "    # Apply low-pass filter\n",
    "    img_filtered = apply_low_pass_filter(img_gray)\n",
    "\n",
    "    # Increase the intensity\n",
    "    img_enhanced = increase_intensity(img_filtered)\n",
    "\n",
    "    # Crop the black boundary\n",
    "    img_cropped = crop_black_boundary(img_enhanced)\n",
    "\n",
    "    img_height, img_width = img_cropped.shape\n",
    "\n",
    "    os.makedirs(target_subdir, exist_ok=True)\n",
    "    target_path = os.path.join(target_subdir, os.path.basename(fn))\n",
    "    cv.imwrite(target_path, img_cropped)\n",
    "\n",
    "    return img_height, img_width\n",
    "\n",
    "def extract_files(base_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Extracts and processes MRI files from a given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    base_dir: Directory containing MRI files.\n",
    "    target_dir: Directory where processed files will be saved.\n",
    "    \"\"\"\n",
    "    HEIGHT = 0\n",
    "    WIDTH = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.png'):\n",
    "                source_path = os.path.join(subdir, file)\n",
    "                relative_path = os.path.relpath(subdir, base_dir)\n",
    "                target_subdir = os.path.join(target_dir, relative_path)\n",
    "                \n",
    "                img_height, img_width = process_image(source_path, target_subdir)\n",
    "                HEIGHT = max(HEIGHT, img_height)\n",
    "                WIDTH = max(WIDTH, img_width)\n",
    "    return HEIGHT, WIDTH\n",
    "\n",
    "if args.process_flag:\n",
    "    in_dir = os.path.join(args.img_dir, 'raw')\n",
    "    out_dir = os.path.join(args.img_dir, 'processed')\n",
    "    extract_files(in_dir, out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage.exposure\n",
    "\n",
    "CONV_WIDTH = 137\n",
    "CONV_HEIGHT = 167\n",
    "\n",
    "def increase_contrast(image):\n",
    "    \"\"\"\n",
    "    Increases the contrast of an image using adaptive histogram equalization.\n",
    "    \"\"\"\n",
    "    # Convert image to float and scale to range 0-1\n",
    "    img_float = image.astype(np.float32) / 255\n",
    "    # Apply adaptive equalization\n",
    "    img_contrast = skimage.exposure.equalize_adapthist(img_float)\n",
    "    # Scale back to range 0-255 and return as uint8\n",
    "    return (img_contrast * 255).astype(np.uint8)\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Reads, enhances contrast, and resizes images from the input directory,\n",
    "    and saves them to the output directory while preserving the subdirectory structure.\n",
    "    \"\"\"\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith('.png'):\n",
    "                file_path = os.path.join(subdir, filename)\n",
    "\n",
    "                # Create corresponding subdirectory in output directory\n",
    "                relative_path = os.path.relpath(subdir, input_dir)\n",
    "                output_subdir = os.path.join(output_dir, relative_path)\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "                # Process the image\n",
    "                img = cv.imread(file_path, cv.IMREAD_GRAYSCALE)\n",
    "                img_enhanced = increase_contrast(img)\n",
    "                img_resized = cv.resize(img_enhanced, (CONV_WIDTH, CONV_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "                # Save the modified image\n",
    "                output_path = os.path.join(output_subdir, filename)\n",
    "                cv.imwrite(output_path, img_resized)\n",
    "\n",
    "if args.process_flag:\n",
    "    in_dir = os.path.join(args.img_dir, 'processed')\n",
    "    out_dir = os.path.join(args.img_dir, 'modified')\n",
    "    process_directory(in_dir, out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming args is defined and has num_classes attribute\n",
    "# args.num_classes = 2 or 3 based on your requirement\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"nondemented\": 0,\n",
    "    \"mildly demented\": 1,\n",
    "    'moderately demented': 1 if args.num_classes == 2 else 2\n",
    "}\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_pids = []\n",
    "    class_counts = Counter()\n",
    "\n",
    "    # Automatically find all subdirectories in base_dir\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "            class_label = LABEL_MAP[folder_name]\n",
    "            for image_file in os.listdir(folder_path):\n",
    "                if image_file == '.DS_Store':\n",
    "                    continue  # Skip .DS_Store files\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                if os.path.isfile(image_path):\n",
    "                    pid = '_'.join(os.path.basename(image_path).split('_')[:3])\n",
    "                    all_pids.append(pid)\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img_tensor = transform(img)\n",
    "                        all_images.append(img_tensor)\n",
    "                        all_labels.append(class_label)\n",
    "                        class_counts[folder_name] += 1\n",
    "\n",
    "    X = torch.stack(all_images)\n",
    "    y = torch.tensor(all_labels, dtype=torch.long)  # Changed to long for integer labels\n",
    "    return X, y, class_counts, all_pids\n",
    "\n",
    "# Example usage\n",
    "# Set args values or replace args.img_dir and args.create_dataset with appropriate values\n",
    "if args.create_dataset:\n",
    "    X, y, class_counts, all_pids = load_dataset(os.path.join(args.img_dir, 'modified'))\n",
    "\n",
    "    print(f\"Combined Tensor Size: {X.size()}\")\n",
    "    print(f\"Labels Tensor Size: {y.size()}\")\n",
    "    print(f\"Class Counts: {class_counts}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Combined Tensor Size: torch.Size([54, 1, 167, 137])\n",
      "Labels Tensor Size: torch.Size([54])\n",
      "Class Counts: Counter({'nondemented': 38, 'mildly demented': 13, 'moderately demented': 3})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "if args.create_dataset:\n",
    "    print(f\"Number of nondemented in train dataset as percentage: {((y == 0).sum() / (X.shape[0])) * 100:0.2f}%\")\n",
    "    print(f\"Number of mildly demented in train dataset as percentage: {((y == 1).sum() / (X.shape[0])) * 100:0.2f}%\")\n",
    "    print(f\"Number of moderately demented in train dataset as percentage: {((y == 2).sum() / (X.shape[0])) * 100:0.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nondemented in train dataset as percentage: 70.37%\n",
      "Number of mildly demented in train dataset as percentage: 24.07%\n",
      "Number of moderately demented in train dataset as percentage: 5.56%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepCNNModel(nn.Module):\n",
    "    def __init__(self, fc_size, conv_in_size, conv_hid_size, conv_out_size, dropout, num_classes=3):\n",
    "        super(DeepCNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(1, conv_in_size, kernel_size=3, padding=1)  \n",
    "        self.bn1 = nn.BatchNorm2d(conv_in_size)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv2d(conv_in_size, conv_hid_size, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(conv_hid_size)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv2d(conv_hid_size, conv_hid_size, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(conv_hid_size)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 4\n",
    "        self.conv4 = nn.Conv2d(conv_hid_size, conv_out_size, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(conv_out_size)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        # Compute the flattened size for the fully connected layer\n",
    "        self._to_linear = None\n",
    "        self._forward_conv(torch.randn(1, 1, 137, 167))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, fc_size)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten the output for the fully connected layers\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def test(model, test_dataset, criterion, batch_size=8, device='cpu'):\n",
    "    \"\"\"\n",
    "    Test the PyTorch model and gather predictions for each class.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained PyTorch model.\n",
    "    test_dataset (torch.utils.data.Dataset): Dataset for testing.\n",
    "    criterion (torch.nn.modules.loss): Loss function.\n",
    "    device (str): Device to run the test ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    float: The average loss over the test dataset.\n",
    "    float: The overall accuracy over the test dataset.\n",
    "    list: A list of tuples, each containing a batch of true labels and predicted labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "            # Save the predictions and the targets for each batch\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_predictions.append(predicted.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "    return avg_loss, accuracy, all_targets, all_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "def calculate_class_weights(y):\n",
    "    # Count the frequency of each class\n",
    "    class_counts = Counter(y.numpy())\n",
    "    total_samples = sum(class_counts.values())\n",
    "\n",
    "    # Calculate weights: Inverse of frequency\n",
    "    weights = {class_id: total_samples/class_counts[class_id] for class_id in class_counts}\n",
    "\n",
    "    # Convert to a list in the order of class ids\n",
    "    weights_list = [weights[i] for i in sorted(weights)]\n",
    "    return torch.tensor(weights_list, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load in config from best run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "import json\n",
    "\n",
    "config_file_path = args.best_custom_model_path.replace('.pt', '.config')\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config_dict = json.load(file)\n",
    "\n",
    "print(config_dict)\n",
    "fc_size = config_dict[\"fc_size\"][\"value\"]\n",
    "conv_in_size = config_dict[\"conv_in_size\"][\"value\"]\n",
    "conv_hid_size = config_dict[\"conv_hid_size\"][\"value\"]\n",
    "conv_out_size = config_dict[\"conv_out_size\"][\"value\"]\n",
    "dropout = config_dict[\"dropout\"][\"value\"]\n",
    "batch_size = config_dict[\"batch_size\"][\"value\"]\n",
    "\n",
    "print(\"fc_size:\", fc_size)\n",
    "print(\"conv_in_size:\", conv_in_size)\n",
    "print(\"conv_hid_size:\", conv_hid_size)\n",
    "print(\"conv_out_size:\", conv_out_size)\n",
    "print(\"dropout:\", dropout)\n",
    "print(\"batch_size:\", batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'lr': {'desc': None, 'value': 0.0001}, '_wandb': {'desc': None, 'value': {'t': {'1': [1, 5, 41, 53, 55], '2': [1, 5, 41, 53, 55], '3': [23, 37], '4': '3.11.4', '5': '0.16.1', '8': [1, 5], '13': 'darwin-x86_64'}, 'framework': 'torch', 'start_time': 1702631742.827628, 'cli_version': '0.16.1', 'is_jupyter_run': True, 'python_version': '3.11.4', 'is_kaggle_kernel': False}}, 'dropout': {'desc': None, 'value': 0.2}, 'fc_size': {'desc': None, 'value': 32}, 'batch_size': {'desc': None, 'value': 8}, 'max_epochs': {'desc': None, 'value': 250}, 'hidden_size': {'desc': None, 'value': 8}, 'conv_in_size': {'desc': None, 'value': 256}, 'conv_hid_size': {'desc': None, 'value': 32}, 'conv_out_size': {'desc': None, 'value': 16}}\n",
      "fc_size: 32\n",
      "conv_in_size: 256\n",
      "conv_hid_size: 32\n",
      "conv_out_size: 16\n",
      "dropout: 0.2\n",
      "batch_size: 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "model = DeepCNNModel(fc_size, conv_in_size, conv_hid_size, conv_out_size, dropout)\n",
    "model.load_state_dict(torch.load(args.best_custom_model_path))\n",
    "test_set = TensorDataset(X, y)\n",
    "criterion = nn.CrossEntropyLoss(weight=calculate_class_weights(y))\n",
    "avg_loss, accuracy, all_targets, all_predictions = test(model, test_set, criterion, batch_size)\n",
    "\n",
    "print(all_targets)\n",
    "print(all_predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([2, 2, 2, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0])]\n",
      "[tensor([1, 1, 1, 1, 1, 0, 0, 1]), tensor([0, 0, 1, 0, 1, 1, 1, 0]), tensor([1, 1, 0, 0, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 1, 0]), tensor([0, 1, 1, 0, 1, 0, 1, 0]), tensor([1, 0, 1, 1, 0, 1, 1, 1]), tensor([1, 0, 1, 0, 0, 0])]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "flat_targets = torch.cat(all_targets).flatten()\n",
    "flat_predictions = torch.cat(all_predictions).flatten()\n",
    "mismatch_indices = torch.where(flat_targets != flat_predictions)[0].tolist()\n",
    "print(f\"Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Loss: 1.0207, Accuracy: 53.70%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "print(flat_targets)\n",
    "print(flat_predictions)\n",
    "print(mismatch_indices)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "[0, 1, 2, 5, 6, 8, 9, 11, 15, 16, 17, 21, 30, 33, 34, 36, 38, 40, 42, 43, 45, 46, 47, 48, 50]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "print(all_pids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['OAS2_0044_MR1', 'OAS2_0066_MR1', 'OAS2_0071_MR1', 'OAS2_0080_MR1', 'OAS2_0063_MR1', 'OAS2_0043_MR1', 'OAS2_0037_MR1', 'OAS2_0089_MR1', 'OAS2_0079_MR1', 'OAS2_0040_MR1', 'OAS2_0032_MR1', 'OAS2_0016_MR1', 'OAS2_0075_MR1', 'OAS2_0058_MR1', 'OAS2_0002_MR1', 'OAS2_0028_MR1', 'OAS2_0077_MR1', 'OAS2_0073_MR1', 'OAS2_0094_MR1', 'OAS2_0029_MR1', 'OAS2_0090_MR1', 'OAS2_0004_MR1', 'OAS2_0013_MR1', 'OAS2_0097_MR1', 'OAS2_0017_MR1', 'OAS2_0070_MR1', 'OAS2_0067_MR1', 'OAS2_0049_MR1', 'OAS2_0054_MR1', 'OAS2_0069_MR1', 'OAS2_0047_MR1', 'OAS2_0027_MR1', 'OAS2_0030_MR1', 'OAS2_0034_MR1', 'OAS2_0057_MR1', 'OAS2_0022_MR1', 'OAS2_0035_MR1', 'OAS2_0031_MR1', 'OAS2_0008_MR1', 'OAS2_0045_MR1', 'OAS2_0052_MR1', 'OAS2_0056_MR1', 'OAS2_0041_MR1', 'OAS2_0078_MR1', 'OAS2_0051_MR1', 'OAS2_0068_MR1', 'OAS2_0042_MR1', 'OAS2_0036_MR1', 'OAS2_0018_MR1', 'OAS2_0001_MR1', 'OAS2_0005_MR1', 'OAS2_0061_MR1', 'OAS2_0091_MR1', 'OAS2_0086_MR1']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "print([all_pids[i] for i in mismatch_indices])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['OAS2_0044_MR1', 'OAS2_0066_MR1', 'OAS2_0071_MR1', 'OAS2_0043_MR1', 'OAS2_0037_MR1', 'OAS2_0079_MR1', 'OAS2_0040_MR1', 'OAS2_0016_MR1', 'OAS2_0028_MR1', 'OAS2_0077_MR1', 'OAS2_0073_MR1', 'OAS2_0004_MR1', 'OAS2_0047_MR1', 'OAS2_0034_MR1', 'OAS2_0057_MR1', 'OAS2_0035_MR1', 'OAS2_0008_MR1', 'OAS2_0052_MR1', 'OAS2_0041_MR1', 'OAS2_0078_MR1', 'OAS2_0068_MR1', 'OAS2_0042_MR1', 'OAS2_0036_MR1', 'OAS2_0018_MR1', 'OAS2_0005_MR1']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test ResNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class GrayscaleToRGBTransform:\n",
    "    def __call__(self, tensor):\n",
    "        # Check if the tensor has one channel (grayscale)\n",
    "        if tensor.shape[0] == 1:\n",
    "            # Repeat the tensor across 3 channels\n",
    "            tensor = tensor.repeat(3, 1, 1)\n",
    "        return tensor\n",
    "\n",
    "res_transform = transforms.Compose([\n",
    "    GrayscaleToRGBTransform(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def apply_all_transforms(X, transform):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = transform(x) \n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "\n",
    "\n",
    "X_test_resnet = apply_all_transforms(X, transform=res_transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, args.num_classes) \n",
    "model.load_state_dict(torch.load(args.best_resnet_model_path))\n",
    "\n",
    "batch_size = 16\n",
    "test_set = TensorDataset(X_test_resnet, y)\n",
    "criterion = nn.CrossEntropyLoss(weight=calculate_class_weights(y))\n",
    "print(\"batch_size:\", batch_size)\n",
    "avg_loss, accuracy, all_targets, all_predictions = test(model, test_set, criterion, batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch_size: 16\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "print(f\"Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Loss: 1.1389, Accuracy: 16.67%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "print(all_targets)\n",
    "print(all_predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0])]\n",
      "[tensor([2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2]), tensor([2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1]), tensor([1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2]), tensor([1, 2, 2, 1, 2, 1])]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.4 64-bit ('nlp-m': conda)"
  },
  "interpreter": {
   "hash": "62ef8ab7a50fde946441678ef251eba85852e5ab7813d8beba261a97f4cf7750"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}