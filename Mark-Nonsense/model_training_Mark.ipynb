{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 558 images belonging to 3 classes.\n",
      "Found 144 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_28 (Conv2D)          (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 111, 111, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPooli  (None, 54, 54, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPooli  (None, 26, 26, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               44302848  \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44397635 (169.36 MB)\n",
      "Trainable params: 44397635 (169.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 6s 292ms/step - loss: 0.9774 - accuracy: 0.5502 - val_loss: 0.9684 - val_accuracy: 0.5625\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 5s 282ms/step - loss: 0.9337 - accuracy: 0.5789 - val_loss: 0.9536 - val_accuracy: 0.5625\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 5s 281ms/step - loss: 0.9077 - accuracy: 0.5753 - val_loss: 0.9616 - val_accuracy: 0.5625\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 5s 281ms/step - loss: 0.9021 - accuracy: 0.5914 - val_loss: 0.9217 - val_accuracy: 0.5556\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 5s 281ms/step - loss: 0.8625 - accuracy: 0.5878 - val_loss: 0.9019 - val_accuracy: 0.6111\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 5s 281ms/step - loss: 0.8259 - accuracy: 0.6344 - val_loss: 0.8709 - val_accuracy: 0.6458\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 5s 283ms/step - loss: 0.7940 - accuracy: 0.6667 - val_loss: 0.9294 - val_accuracy: 0.5694\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 5s 290ms/step - loss: 0.8076 - accuracy: 0.6452 - val_loss: 0.9132 - val_accuracy: 0.5625\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 5s 285ms/step - loss: 0.7416 - accuracy: 0.6756 - val_loss: 0.8415 - val_accuracy: 0.6319\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 5s 286ms/step - loss: 0.7005 - accuracy: 0.6792 - val_loss: 0.8486 - val_accuracy: 0.5694\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 5s 288ms/step - loss: 0.6682 - accuracy: 0.7151 - val_loss: 0.8265 - val_accuracy: 0.6319\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 5s 286ms/step - loss: 0.6069 - accuracy: 0.7366 - val_loss: 0.8124 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 5s 288ms/step - loss: 0.5636 - accuracy: 0.7563 - val_loss: 0.9387 - val_accuracy: 0.5833\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 5s 287ms/step - loss: 0.5403 - accuracy: 0.7634 - val_loss: 0.8587 - val_accuracy: 0.5903\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 5s 293ms/step - loss: 0.4831 - accuracy: 0.8172 - val_loss: 0.8499 - val_accuracy: 0.5903\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 5s 289ms/step - loss: 0.4231 - accuracy: 0.8369 - val_loss: 0.9130 - val_accuracy: 0.5972\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.3972 - accuracy: 0.8584 - val_loss: 0.9720 - val_accuracy: 0.5903\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.3450 - accuracy: 0.8763 - val_loss: 1.0586 - val_accuracy: 0.6319\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 5s 292ms/step - loss: 0.2688 - accuracy: 0.9194 - val_loss: 1.0637 - val_accuracy: 0.6181\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.2487 - accuracy: 0.9176 - val_loss: 1.0508 - val_accuracy: 0.5625\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 5s 288ms/step - loss: 0.2239 - accuracy: 0.9391 - val_loss: 1.2540 - val_accuracy: 0.5903\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 5s 290ms/step - loss: 0.2026 - accuracy: 0.9391 - val_loss: 1.1420 - val_accuracy: 0.6181\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 5s 289ms/step - loss: 0.1772 - accuracy: 0.9444 - val_loss: 1.1431 - val_accuracy: 0.5833\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 5s 301ms/step - loss: 0.1294 - accuracy: 0.9695 - val_loss: 1.1853 - val_accuracy: 0.5972\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.1228 - accuracy: 0.9588 - val_loss: 1.1408 - val_accuracy: 0.6111\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 5s 301ms/step - loss: 0.0942 - accuracy: 0.9839 - val_loss: 1.1998 - val_accuracy: 0.6111\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 5s 292ms/step - loss: 0.0733 - accuracy: 0.9892 - val_loss: 1.4189 - val_accuracy: 0.6111\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 6s 310ms/step - loss: 0.0667 - accuracy: 0.9857 - val_loss: 1.3416 - val_accuracy: 0.5972\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 5s 290ms/step - loss: 0.0526 - accuracy: 0.9946 - val_loss: 1.3249 - val_accuracy: 0.6250\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 5s 293ms/step - loss: 0.0411 - accuracy: 0.9946 - val_loss: 1.4546 - val_accuracy: 0.6111\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 5s 292ms/step - loss: 0.0540 - accuracy: 0.9892 - val_loss: 1.3453 - val_accuracy: 0.5764\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.0383 - accuracy: 0.9982 - val_loss: 1.4722 - val_accuracy: 0.6042\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 5s 295ms/step - loss: 0.0298 - accuracy: 0.9982 - val_loss: 1.4467 - val_accuracy: 0.5903\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 5s 294ms/step - loss: 0.0261 - accuracy: 0.9946 - val_loss: 1.4384 - val_accuracy: 0.5764\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.0296 - accuracy: 0.9982 - val_loss: 1.5054 - val_accuracy: 0.6042\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 5s 295ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 1.7175 - val_accuracy: 0.6250\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 5s 294ms/step - loss: 0.0147 - accuracy: 0.9964 - val_loss: 1.6622 - val_accuracy: 0.5972\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 5s 291ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.5548 - val_accuracy: 0.5903\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 5s 296ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.6826 - val_accuracy: 0.6111\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 5s 293ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.6545 - val_accuracy: 0.5903\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 5s 293ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.7700 - val_accuracy: 0.6042\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 5s 296ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.7487 - val_accuracy: 0.5972\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 5s 292ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.7603 - val_accuracy: 0.5972\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 5s 295ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.7413 - val_accuracy: 0.5972\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 5s 292ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.0348 - val_accuracy: 0.6389\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 5s 295ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.7032 - val_accuracy: 0.6042\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 5s 303ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.9247 - val_accuracy: 0.6111\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 5s 296ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.8160 - val_accuracy: 0.5764\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 5s 296ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.8820 - val_accuracy: 0.5694\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 5s 295ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.8162 - val_accuracy: 0.6111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2de594a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 3  # mildly demented, moderately demented, nondemented\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "data_dir = './notebooks/'\n",
    "\n",
    "def filter_directories(directory):\n",
    "    \"\"\" \n",
    "    Returns a list of directories that contain all three image types: 'cor', 'sag', 'tra'.\n",
    "    \"\"\"\n",
    "    filtered_dirs = []\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for patient_dir in os.listdir(class_path):\n",
    "                patient_path = os.path.join(class_path, patient_dir)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    image_types = os.listdir(patient_path)\n",
    "                    if all(x in image_types for x in ['tra', 'cor', 'sag']):\n",
    "                        filtered_dirs.append(patient_path)\n",
    "    return filtered_dirs\n",
    "\n",
    "# Modify the ImageDataGenerator to not use the 'validation_split'\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use the directory filter function to get the list of valid directories\n",
    "valid_training_dirs = filter_directories(data_dir + 'training')\n",
    "valid_validation_dirs = filter_directories(data_dir + 'testing')\n",
    "\n",
    "# Create a custom generator to handle the multi-directory structure\n",
    "def custom_generator(generator, directories):\n",
    "    while True:\n",
    "        for directory in directories:\n",
    "            for x, y in generator.flow_from_directory(\n",
    "                directory,\n",
    "                target_size=(img_height, img_width),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical'  # as we have multiple classes\n",
    "            ):\n",
    "                yield x, y\n",
    "\n",
    "\n",
    "# Update directory paths\n",
    "# Updated directory paths\n",
    "train_dir = './notebooks/training/'  # Replace with the path to your training data\n",
    "val_dir = './notebooks/testing/'  # Replace with the path to your validation data\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(validation_generator)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,  # You can adjust the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 3  # mildly demented, moderately demented, nondemented\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "data_dir = './data_split/'\n",
    "\n",
    "def filter_directories(directory):\n",
    "    \"\"\" \n",
    "    Returns a list of directories that contain all three image types: 'cor', 'sag', 'tra'.\n",
    "    \"\"\"\n",
    "    filtered_dirs = []\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for patient_dir in os.listdir(class_path):\n",
    "                patient_path = os.path.join(class_path, patient_dir)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    image_types = os.listdir(patient_path)\n",
    "                    if all(x in image_types for x in ['tra', 'cor', 'sag']):\n",
    "                        filtered_dirs.append(patient_path)\n",
    "    return filtered_dirs\n",
    "\n",
    "# Modify the ImageDataGenerator to not use the 'validation_split'\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use the directory filter function to get the list of valid directories\n",
    "valid_training_dirs = filter_directories(data_dir + 'training')\n",
    "valid_validation_dirs = filter_directories(data_dir + 'validation')\n",
    "\n",
    "def custom_generator(directory, batch_size, target_size):\n",
    "    while True:\n",
    "        # Get all image paths and corresponding class names\n",
    "        image_paths = []\n",
    "        class_names = []\n",
    "        for class_dir in os.listdir(directory):\n",
    "            class_path = os.path.join(directory, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        image_paths.append(os.path.join(class_path, img_name))\n",
    "                        class_names.append(class_dir)\n",
    "\n",
    "        # Shuffle the data\n",
    "        combined = list(zip(image_paths, class_names))\n",
    "        np.random.shuffle(combined)\n",
    "        image_paths[:], class_names[:] = zip(*combined)\n",
    "\n",
    "        # Generate batches\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_classes = class_names[i:i+batch_size]\n",
    "\n",
    "            images = []\n",
    "            labels = []\n",
    "            for j, path in enumerate(batch_paths):\n",
    "                try:\n",
    "                    img = cv2.imread(path)\n",
    "                    img = cv2.resize(img, target_size)\n",
    "                    images.append(img)\n",
    "                    labels.append(batch_classes[j])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file: {path}, Error: {e}\")\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            images_np = np.array(images, dtype='float32') / 255.0\n",
    "            labels_np = np.array([class_names.index(c) for c in labels])\n",
    "\n",
    "            yield images_np, labels_np\n",
    "\n",
    "# Update directory paths\n",
    "train_dir = './data_split/data_train/training/'\n",
    "val_dir = './data_split/data_val/validation/'\n",
    "\n",
    "# Image Data Generator\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(validation_generator)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=25,  # You can adjust the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Updated directory paths\n",
    "train_dir = './notebooks/training/'  # Replace with the path to your training data\n",
    "val_dir = './notebooks/testing/'  # Replace with the path to your validation data\n",
    "\n",
    "# # Randomly choose between training and validation directories\n",
    "# chosen_dir = random.choice([train_dir, val_dir])\n",
    "\n",
    "\n",
    "# Modify the generator to pull a 3D input (batch stacking and take 4D input (1st time, 2D image,.....) :(\n",
    "# (Do a cut off /truncate) , (1st, middle, last)\n",
    "# Batch 60 (depends on compressing (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the classes in the chosen directory\n",
    "classes = os.listdir(chosen_dir)\n",
    "random_class = random.choice(classes)\n",
    "class_dir = os.path.join(chosen_dir, random_class)\n",
    "\n",
    "# List the images in the chosen class directory\n",
    "image_files = [f for f in os.listdir(class_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "random_image_file = random.choice(image_files)\n",
    "image_path = os.path.join(class_dir, random_image_file)\n",
    "\n",
    "# Modify the generator to pull a 3D input (batch stacking and take 4D input (1st time, 2D image,.....) :(\n",
    "# (Do a cut off /truncate) , (1st, middle, last)\n",
    "# Batch 60 (depends on compressing (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image\n",
    "    img_resized = cv2.resize(img, target_size)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_normalized = img_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to array and add an extra dimension\n",
    "    img_array = np.expand_dims(img_normalized, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Function to get 10 random images from the directory\n",
    "def get_random_images(directory, num_images=10):\n",
    "    all_images = []\n",
    "    \n",
    "    # Get all class subdirectories\n",
    "    class_dirs = [os.path.join(directory, d) for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    \n",
    "    while len(all_images) < num_images and class_dirs:\n",
    "        # Choose a random class directory\n",
    "        class_dir = random.choice(class_dirs)\n",
    "        \n",
    "        # Get all image files in this directory\n",
    "        image_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # If there are no images left in this class, remove the class and continue\n",
    "        if not image_files:\n",
    "            class_dirs.remove(class_dir)\n",
    "            continue\n",
    "        \n",
    "        # Randomly pick an image\n",
    "        image_path = random.choice(image_files)\n",
    "        all_images.append(preprocess_image(image_path))\n",
    "        \n",
    "        # Optional: remove the chosen image from the list if you want no repetition\n",
    "        image_files.remove(image_path)\n",
    "    \n",
    "    return np.vstack(all_images)\n",
    "\n",
    "# Choose between training and validation directories\n",
    "chosen_dir = random.choice([train_dir, val_dir])\n",
    "\n",
    "# Get 10 random preprocessed images\n",
    "images_array = get_random_images(chosen_dir, num_images=10)\n",
    "print(f\"Processed {len(images_array)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(images_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'images_array' contains the batch of preprocessed images\n",
    "\n",
    "# Label encoding setup\n",
    "class_labels = ['Mildly Demented', 'Moderately Demented', 'Nondemented']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(class_labels)\n",
    "\n",
    "# Define a function to compute Grad-CAM\n",
    "def compute_grad_cam(img_tensor, model, last_conv_layer_name, classifier_layer_names):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output] + [model.get_layer(name).output for name in classifier_layer_names]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_tensor)\n",
    "        if preds.ndim == 1:\n",
    "            preds = tf.expand_dims(preds, axis=0)\n",
    "        class_idx = tf.argmax(preds[0])\n",
    "        loss = preds[:, class_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, last_conv_layer_output)\n",
    "\n",
    "    # Global Average Pooling of gradients\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For numerical stability\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Process each image and display Grad-CAM\n",
    "for i in range(len(images_array)):\n",
    "    img_tensor = images_array[i:i+1]  # Select the i-th image for processing\n",
    "    \n",
    "    # Compute Grad-CAM\n",
    "    heatmap = compute_grad_cam(img_tensor, model, 'conv2d_2', ['dense'])\n",
    "\n",
    "    # Use cv2 to apply the heatmap to the original image\n",
    "    heatmap = cv2.resize(heatmap, (img_tensor.shape[1], img_tensor.shape[2]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Superimpose the heatmap on the original image\n",
    "    superimposed_img = heatmap * 0.4 + img_tensor[0]\n",
    "    superimposed_img = np.uint8(superimposed_img)\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the image with Grad-CAM heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    predicted_class_index = np.argmax(model.predict(img_tensor)[0])\n",
    "    predicted_class_label = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title(f\"Image {i+1} - Predicted Class: {predicted_class_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image\n",
    "    img_resized = cv2.resize(img, target_size)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_normalized = img_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to array and add an extra dimension\n",
    "    img_array = np.expand_dims(img_normalized, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Function to get 10 random images from the directory\n",
    "def get_random_images(directory, num_images=10):\n",
    "    all_images = []\n",
    "    \n",
    "    # Get all class subdirectories\n",
    "    class_dirs = [os.path.join(directory, d) for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    \n",
    "    while len(all_images) < num_images and class_dirs:\n",
    "        # Choose a random class directory\n",
    "        class_dir = random.choice(class_dirs)\n",
    "        \n",
    "        # Get all image files in this directory\n",
    "        image_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # If there are no images left in this class, remove the class and continue\n",
    "        if not image_files:\n",
    "            class_dirs.remove(class_dir)\n",
    "            continue\n",
    "        \n",
    "        # Randomly pick an image\n",
    "        image_path = random.choice(image_files)\n",
    "        all_images.append(preprocess_image(image_path))\n",
    "        \n",
    "        # Optional: remove the chosen image from the list if you want no repetition\n",
    "        image_files.remove(image_path)\n",
    "    \n",
    "    return np.vstack(all_images)\n",
    "\n",
    "# Choose between training and validation directories\n",
    "chosen_dir = random.choice([train_dir, val_dir])\n",
    "\n",
    "# Get 10 random preprocessed images\n",
    "images_array = get_random_images(chosen_dir, num_images=10)\n",
    "print(f\"Processed {len(images_array)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function definitions remain the same...\n",
    "\n",
    "# Label encoding setup\n",
    "class_labels = ['Mildly Demented', 'Moderately Demented', 'Nondemented']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(class_labels)\n",
    "\n",
    "# Choose between training and validation directories\n",
    "chosen_dir = random.choice([train_dir, val_dir])\n",
    "\n",
    "# Get 10 random preprocessed images\n",
    "images_array, resized_images = get_random_images(chosen_dir, num_images=10)\n",
    "print(f\"Processed {len(images_array)} images.\")\n",
    "\n",
    "# Replace with 'conv2d_2' to use the same layer as in the first snippet\n",
    "conv_layer_name = 'conv2d_46'\n",
    "last_conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "# Create a model with the same inputs as your original model and outputs of the last conv layer\n",
    "grad_model = tf.keras.models.Model([model.inputs], [last_conv_layer.output, model.output])\n",
    "\n",
    "# Generate Grad-CAM for each image\n",
    "for i in range(len(images_array)):\n",
    "    img_tensor = tf.convert_to_tensor(images_array[i:i+1])  # Convert to a TensorFlow tensor\n",
    "    img_resized = resized_images[i]  # Corresponding resized image\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img_tensor)  # Ensure the image tensor is being watched\n",
    "        conv_outputs, predictions = grad_model(img_tensor)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "\n",
    "    output = conv_outputs[0]\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "    # Guided gradients - use positive gradients only for the chosen class\n",
    "    cast_conv_outputs = tf.cast(output > 0, 'float32')\n",
    "    cast_grads = tf.cast(grads > 0, 'float32')\n",
    "    guided_grads = cast_conv_outputs * cast_grads * grads\n",
    "\n",
    "    # Weighted feature map - spatial average of the gradients\n",
    "    weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    cam = cv2.resize(cam, (224, 224))  # Resize to image size\n",
    "    cam = np.maximum(cam, 0)  # ReLU\n",
    "    heatmap = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Apply heatmap to original image\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img_resized\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Display the image with Grad-CAM heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    predicted_class_index = class_idx.numpy()\n",
    "    predicted_class_label = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "    plt.imshow(superimposed_img[:,:,::-1])\n",
    "    plt.title(f\"Image {i+1} - Predicted Class: {predicted_class_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Take 3 images from each class and see how the contour (heatmap) changes overtime with the training\n",
    "    # Once the model is trained do a side-by-side image of the original and the heatmap image to see the difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 3  # mildly demented, moderately demented, nondemented\n",
    "\n",
    "# Load VGG16 pre-trained on ImageNet data\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a custom head for our dataset\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Directory paths\n",
    "data_dir = './data_split/'\n",
    "\n",
    "# Function to filter directories\n",
    "def filter_directories(directory):\n",
    "    filtered_dirs = []\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for patient_dir in os.listdir(class_path):\n",
    "                patient_path = os.path.join(class_path, patient_dir)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    image_types = os.listdir(patient_path)\n",
    "                    if all(x in image_types for x in ['tra', 'cor', 'sag']):\n",
    "                        filtered_dirs.append(patient_path)\n",
    "    return filtered_dirs\n",
    "\n",
    "# Image Data Generators\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Training and Validation Directories\n",
    "train_dir = './data_split/data_train/training/'\n",
    "val_dir = './data_split/data_val/validation/'\n",
    "\n",
    "# Training and Validation Generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(validation_generator)\n",
    "\n",
    "# Train the model and save the training history\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=25,  # Adjust the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.patches as mpatches\n",
    "# Assume 'model' is your pre-trained model\n",
    "\n",
    "# Subset size - adjust this to the actual number of images you're using for t-SNE\n",
    "subset_size = 685\n",
    "\n",
    "# Extract features for a subset of your dataset\n",
    "# Ensure that images_array is correctly prepared for the subset\n",
    "if len(images_array) != subset_size:\n",
    "    raise ValueError(f\"The size of images_array ({len(images_array)}) does not match the subset_size ({subset_size}).\")\n",
    "\n",
    "print(images_array)\n",
    "print(subset_size)\n",
    "# Modify the model to create a feature extractor\n",
    "# If 'dense' is the name of the last dense layer before the output\n",
    "feature_extractor = Model(inputs=model.inputs, outputs=model.get_layer('dense_8').output)\n",
    "\n",
    "# Collect features for the subset\n",
    "features = feature_extractor.predict(images_array)\n",
    "\n",
    "# Apply t-SNE reduction with adjusted perplexity\n",
    "tsne_perplexity = min(90, subset_size - 1)  # Perplexity must be less than the number of samples\n",
    "\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=tsne_perplexity, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# Adjust class counts based on the subset size\n",
    "num_mildly_demented = subset_size // 3\n",
    "num_moderately_demented = subset_size // 3\n",
    "num_nondemented = subset_size - (num_mildly_demented + num_moderately_demented)\n",
    "\n",
    "# Create the class_names array for the subset\n",
    "class_names = (['mildly demented'] * num_mildly_demented +\n",
    "               ['moderately demented'] * num_moderately_demented +\n",
    "               ['nondemented'] * num_nondemented)\n",
    "\n",
    "# Ensure the class_names array length matches the subset size\n",
    "assert len(class_names) == subset_size, \"Number of class names must match number of images in the subset\"\n",
    "\n",
    "# Encode the class names into numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(class_names)\n",
    "\n",
    "\n",
    "# Plot the t-SNE results for the subset\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='viridis')\n",
    "plt.colorbar(scatter, ticks=range(len(np.unique(labels))))\n",
    "plt.title('t-SNE of Feature Space')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "# Create legend handles manually\n",
    "unique_labels = np.unique(labels)\n",
    "legend_labels = label_encoder.inverse_transform(unique_labels)\n",
    "legend_handles = [mpatches.Patch(color=plt.cm.viridis(i / len(unique_labels)), label=label) for i, label in enumerate(legend_labels)]\n",
    "\n",
    "# Use only the handles for the legend\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.show()\n",
    "\n",
    "# CNN is performing some transformation of features and comapre before and after image\n",
    "# OK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses a custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 3  # mildly demented, moderately demented, nondemented\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "data_dir = './data_split/'\n",
    "\n",
    "def filter_directories(directory):\n",
    "    \"\"\" \n",
    "    Returns a list of directories that contain all three image types: 'cor', 'sag', 'tra'.\n",
    "    \"\"\"\n",
    "    filtered_dirs = []\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for patient_dir in os.listdir(class_path):\n",
    "                patient_path = os.path.join(class_path, patient_dir)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    image_types = os.listdir(patient_path)\n",
    "                    if all(x in image_types for x in ['tra', 'cor', 'sag']):\n",
    "                        filtered_dirs.append(patient_path)\n",
    "    return filtered_dirs\n",
    "\n",
    "# Modify the ImageDataGenerator to not use the 'validation_split'\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use the directory filter function to get the list of valid directories\n",
    "valid_training_dirs = filter_directories(data_dir + 'training')\n",
    "valid_validation_dirs = filter_directories(data_dir + 'validation')\n",
    "\n",
    "def custom_generator(directory, batch_size, target_size):\n",
    "    while True:\n",
    "        # Get all image paths and corresponding class names\n",
    "        image_paths = []\n",
    "        class_names = []\n",
    "        for class_dir in os.listdir(directory):\n",
    "            class_path = os.path.join(directory, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        image_paths.append(os.path.join(class_path, img_name))\n",
    "                        class_names.append(class_dir)\n",
    "\n",
    "        # Shuffle the data\n",
    "        combined = list(zip(image_paths, class_names))\n",
    "        np.random.shuffle(combined)\n",
    "        image_paths[:], class_names[:] = zip(*combined)\n",
    "\n",
    "        # Generate batches\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_classes = class_names[i:i+batch_size]\n",
    "\n",
    "            images = []\n",
    "            labels = []\n",
    "            for j, path in enumerate(batch_paths):\n",
    "                try:\n",
    "                    img = cv2.imread(path)\n",
    "                    img = cv2.resize(img, target_size)\n",
    "                    images.append(img)\n",
    "                    labels.append(batch_classes[j])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file: {path}, Error: {e}\")\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            images_np = np.array(images, dtype='float32') / 255.0\n",
    "            labels_np = np.array([class_names.index(c) for c in labels])\n",
    "\n",
    "            yield images_np, labels_np\n",
    "\n",
    "# Update directory paths\n",
    "train_dir = './data_split/data_train/training/'\n",
    "val_dir = './data_split/data_val/validation/'\n",
    "\n",
    "# Image Data Generator\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(validation_generator)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=25,  # You can adjust the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the image and mask generator\n",
    "def image_mask_generator(image_dir, mask_dir, batch_size, target_size):\n",
    "    image_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        image_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        color_mode='rgb',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        mask_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        imgs_batch = image_generator.next()\n",
    "        masks_batch = mask_generator.next()\n",
    "\n",
    "        # Ensure batch sizes are equal\n",
    "        min_batch_size = min(len(imgs_batch), len(masks_batch))\n",
    "        if min_batch_size < batch_size:\n",
    "            # If last batch is smaller, truncate both batches to the same size\n",
    "            imgs_batch = imgs_batch[:min_batch_size]\n",
    "            masks_batch = masks_batch[:min_batch_size]\n",
    "\n",
    "        yield imgs_batch, masks_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update directory paths\n",
    "# train_image_dir = './data_split/data_train/training/'\n",
    "train_mask_dir = './FSL_data_split/training/'\n",
    "\n",
    "# # Updated directory paths\n",
    "train_image_dir = './notebooks/training/'  # Replace with the path to your training data\n",
    "# val_mask_dir = './notebooks/testing/'  # Replace with the path to your validation data\n",
    "\n",
    "# Set batch size and target size\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Create the generator for training data\n",
    "train_generator = image_mask_generator(train_image_dir, train_mask_dir, batch_size, (img_height, img_width))\n",
    "\n",
    "# Get a batch of images and masks\n",
    "imgs_batch, masks_batch = next(train_generator)\n",
    "print(\"Shape of images batch:\", imgs_batch.shape)\n",
    "print(\"Shape of masks batch:\", masks_batch.shape)\n",
    "\n",
    "# Display the first image and mask from the batch\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(imgs_batch[0])\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(masks_batch) > 0 and masks_batch[0].shape[-1] == 1:\n",
    "    plt.imshow(masks_batch[0].squeeze(), cmap='gray')\n",
    "    plt.title('Mask')\n",
    "else:\n",
    "    print(\"Invalid mask data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_masks = next(train_generator)\n",
    "print(\"Test Image batch shape:\", test_images.shape)\n",
    "print(\"Test Mask batch shape:\", test_masks.shape)\n",
    "print(os.listdir(train_mask_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the training generator\n",
    "for i, (images, masks) in enumerate(train_generator):\n",
    "    # Only process a certain number of batches\n",
    "    if i >= 10:  # Adjust this number to process more or fewer batches\n",
    "        break\n",
    "\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"  Masks shape: {masks.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(input_tensor, num_filters, dropout_rate=None, batch_norm=False):\n",
    "    x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def upsample_concat(block, bridge):\n",
    "    up = Conv2DTranspose(block.shape[-1], (2, 2), strides=(2, 2), padding='same')(block)\n",
    "    return concatenate([up, bridge])\n",
    "\n",
    "def unet_model(input_size=(256, 256, 3), dropout_rate=0.3, batch_norm=True):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = conv_block(inputs, 16, dropout_rate, batch_norm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    c2 = conv_block(p1, 32)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    c3 = conv_block(p2, 64)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    c4 = conv_block(p3, 128)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = conv_block(p4, 256)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = upsample_concat(b, c4)\n",
    "    c5 = conv_block(u1, 128)\n",
    "    u2 = upsample_concat(c5, c3)\n",
    "    c6 = conv_block(u2, 64)\n",
    "    u3 = upsample_concat(c6, c2)\n",
    "    c7 = conv_block(u3, 32)\n",
    "    u4 = upsample_concat(c7, c1)\n",
    "    c8 = conv_block(u4, 16)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c8)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Class weights calculated from your dataset\n",
    "class_weights = [0.5244582730517705, 1.8019145230885272, 1.8576817573005246]  # Training weights\n",
    "# {0: 0.5244582730517705, 1: 1.8019145230885272, 2: 1.8576817573005246}\n",
    "# Custom loss function with class weights\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    class_weights_tensor = tf.constant(class_weights)\n",
    "    weights = tf.reduce_sum(class_weights_tensor * y_true, axis=-1)\n",
    "    unweighted_losses = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_losses = unweighted_losses * weights\n",
    "    return tf.reduce_mean(weighted_losses)\n",
    "\n",
    "# Initialize the U-Net model and compile with custom loss\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss=weighted_cross_entropy, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Initialize the U-Net model\n",
    "model = unet_model()\n",
    "\n",
    "# Updated directory paths\n",
    "train_image_dir = './notebooks/training/'  # Replace with the path to your training data\n",
    "val_mask_dir = './notebooks/testing/'  # Replace with the path to your validation data\n",
    "\n",
    "\n",
    "# Corrected directory paths\n",
    "# train_image_dir = './data_split/data_train/training'\n",
    "train_mask_dir = './FSL_data_split/training'\n",
    "val_image_dir = './data_split/data_test/testing'\n",
    "# val_mask_dir = './FSL_data_split/validation'\n",
    "\n",
    "# Create generators\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "train_generator = image_mask_generator(train_image_dir, train_mask_dir, batch_size, (img_height, img_width))\n",
    "val_generator = image_mask_generator(val_image_dir, val_mask_dir, batch_size, (img_height, img_width))\n",
    "\n",
    "# Calculate steps per epoch for training and validation\n",
    "train_image_count = 685  # Update with actual count\n",
    "val_image_count = 231    # Update with actual count\n",
    "steps_per_epoch = train_image_count // batch_size\n",
    "validation_steps = val_image_count // batch_size\n",
    "\n",
    "# Adjust steps if there are remaining images\n",
    "if train_image_count % batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "if val_image_count % batch_size != 0:\n",
    "    validation_steps += 1\n",
    "\n",
    "# Training the model\n",
    "epochs = 20  # Adjust this value\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    # learning_rate=0.001,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def count_pixels_in_masks(directory):\n",
    "#     class_counts = [0, 0, 0]  # Assuming three classes: 0, 1, 2\n",
    "\n",
    "#     for foldername in os.listdir(directory):\n",
    "#         folder_path = os.path.join(directory, foldername)\n",
    "#         if os.path.isdir(folder_path):\n",
    "#             for filename in os.listdir(folder_path):\n",
    "#                 if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                     filepath = os.path.join(folder_path, filename)\n",
    "#                     mask = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "#                     if mask is not None:\n",
    "#                         for i in range(3):  # Assuming three classes\n",
    "#                             class_counts[i] += np.sum(mask == i)\n",
    "\n",
    "#     return class_counts\n",
    "\n",
    "# # Directories\n",
    "# train_dir = './data_split/data_train/training/'\n",
    "# val_dir = './data_split/data_val/validation/'\n",
    "# fsl_train_dir = './FSL_data_split/training/'\n",
    "# fsl_val_dir = './FSL_data_split/validation/'\n",
    "\n",
    "# # Count pixels in each class\n",
    "# train_counts = count_pixels_in_masks(train_dir)\n",
    "# val_counts = count_pixels_in_masks(val_dir)\n",
    "# fsl_train_counts = count_pixels_in_masks(fsl_train_dir)\n",
    "# fsl_val_counts = count_pixels_in_masks(fsl_val_dir)\n",
    "\n",
    "# # Combine counts from training and validation sets\n",
    "# combined_counts = [train_counts[i] + val_counts[i] + fsl_train_counts[i] + fsl_val_counts[i] for i in range(3)]\n",
    "\n",
    "# print(\"Combined pixel counts for each class:\", combined_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def count_pixels_in_class(directory):\n",
    "    class_counts = [0, 0, 0]  # Adjust this if you have more classes\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            mask = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is not None:\n",
    "                for i in range(len(class_counts)):\n",
    "                    class_counts[i] += np.sum(mask == i)\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Directories for FSL masks\n",
    "fsl_train_dir = './FSL_data_split/training/'  # Update with actual path\n",
    "fsl_val_dir = './FSL_data_split/validation/'  # Update with actual path\n",
    "\n",
    "# # Directories for regular masks\n",
    "# regular_train_dir = './data_split/data_train/training/masks'  # Update with actual path\n",
    "# regular_val_dir = './data_split/data_val/validation/masks'  # Update with actual path\n",
    "\n",
    "# Calculate pixel counts\n",
    "fsl_train_counts = count_pixels_in_class(fsl_train_dir)\n",
    "fsl_val_counts = count_pixels_in_class(fsl_val_dir)\n",
    "# regular_train_counts = count_pixels_in_class(regular_train_dir)\n",
    "# regular_val_counts = count_pixels_in_class(regular_val_dir)\n",
    "\n",
    "# Output results\n",
    "print(\"FSL Training Counts:\", fsl_train_counts)\n",
    "print(\"FSL Validation Counts:\", fsl_val_counts)\n",
    "# print(\"Regular Training Counts:\", regular_train_counts)\n",
    "# print(\"Regular Validation Counts:\", regular_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pixels_in_class(directory):\n",
    "    class_counts = [0, 0, 0]  # [nondemented, mildly demented, moderately demented]\n",
    "    debug_info = {}\n",
    "\n",
    "    for class_folder in os.listdir(directory):\n",
    "        class_folder_path = os.path.join(directory, class_folder)\n",
    "        debug_info[class_folder] = {'files_processed': 0, 'unique_pixel_values': set()}\n",
    "\n",
    "        if os.path.isdir(class_folder_path):\n",
    "            for filename in os.listdir(class_folder_path):\n",
    "                filepath = os.path.join(class_folder_path, filename)\n",
    "                if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    mask = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "                    if mask is not None:\n",
    "                        debug_info[class_folder]['files_processed'] += 1\n",
    "                        unique_values = np.unique(mask)\n",
    "                        debug_info[class_folder]['unique_pixel_values'].update(unique_values)\n",
    "\n",
    "                        # Update class counts based on your encoding\n",
    "                        class_counts[0] += np.sum(mask == 0)  # nondemented\n",
    "                        class_counts[1] += np.sum(mask == 127)  # mildly demented\n",
    "                        class_counts[2] += np.sum(mask == 191)  # moderately demented\n",
    "\n",
    "    return class_counts, debug_info\n",
    "\n",
    "\n",
    "# Directories for FSL masks\n",
    "fsl_train_dir = './FSL_data_split/training'  # Update with actual path\n",
    "fsl_val_dir = './FSL_data_split/validation'  # Update with actual path\n",
    "\n",
    "# Calculate pixel counts\n",
    "fsl_train_counts, fsl_train_debug = count_pixels_in_class(fsl_train_dir)\n",
    "fsl_val_counts, fsl_val_debug = count_pixels_in_class(fsl_val_dir)\n",
    "\n",
    "# Output results\n",
    "print(\"FSL Training Counts:\", fsl_train_counts)\n",
    "print(\"FSL Training Debug Info:\", fsl_train_debug)\n",
    "print(\"FSL Validation Counts:\", fsl_val_counts)\n",
    "print(\"FSL Validation Debug Info:\", fsl_val_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels_train = sum(fsl_train_counts)\n",
    "total_pixels_val = sum(fsl_val_counts)\n",
    "\n",
    "# Calculate weights for each class\n",
    "class_weights_train = {i: total_pixels_train / (3.0 * count) for i, count in enumerate(fsl_train_counts)}\n",
    "class_weights_val = {i: total_pixels_val / (3.0 * count) for i, count in enumerate(fsl_val_counts)}\n",
    "\n",
    "print(\"Class weights for Training:\", class_weights_train)\n",
    "print(\"Class weights for Validation:\", class_weights_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def count_pixels_in_class(directory):\n",
    "    class_counts = [0, 0, 0]  # Adjust this if you have more classes\n",
    "    debug_info = {}\n",
    "\n",
    "    # Iterate through each subfolder representing a class\n",
    "    for class_folder in os.listdir(directory):\n",
    "        class_folder_path = os.path.join(directory, class_folder)\n",
    "        debug_info[class_folder] = {'files_processed': 0, 'unique_pixel_values': set()}\n",
    "\n",
    "        if os.path.isdir(class_folder_path):\n",
    "            for filename in os.listdir(class_folder_path):\n",
    "                filepath = os.path.join(class_folder_path, filename)\n",
    "                if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    mask = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "                    if mask is not None:\n",
    "                        debug_info[class_folder]['files_processed'] += 1\n",
    "                        unique_values = np.unique(mask)\n",
    "                        debug_info[class_folder]['unique_pixel_values'].update(unique_values)\n",
    "\n",
    "                        for i in range(len(class_counts)):\n",
    "                            class_counts[i] += np.sum(mask == i)\n",
    "\n",
    "    return class_counts, debug_info\n",
    "\n",
    "# Directories for FSL masks\n",
    "fsl_train_dir = './FSL_data_split/training'  # Update with actual path\n",
    "fsl_val_dir = './FSL_data_split/validation'  # Update with actual path\n",
    "\n",
    "# Calculate pixel counts\n",
    "fsl_train_counts, fsl_train_debug = count_pixels_in_class(fsl_train_dir)\n",
    "fsl_val_counts, fsl_val_debug = count_pixels_in_class(fsl_val_dir)\n",
    "\n",
    "# Output results\n",
    "print(\"FSL Training Counts:\", fsl_train_counts)\n",
    "print(\"FSL Training Debug Info:\", fsl_train_debug)\n",
    "print(\"FSL Validation Counts:\", fsl_val_counts)\n",
    "print(\"FSL Validation Debug Info:\", fsl_val_debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define your dataset directory paths\n",
    "# train_dir = './data_split/data_train/training/'\n",
    "# val_dir = './data_split/data_val/validation/'\n",
    "train_dir = './FSL_data_split/training/'\n",
    "val_dir = './FSL_data_split/validation/'\n",
    "# Function to count images in each class directory\n",
    "def count_images_in_directory(directory):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            image_count = len([name for name in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, name))])\n",
    "            class_counts[class_name] = image_count\n",
    "    return class_counts\n",
    "\n",
    "# Count images in the training and validation directories separately\n",
    "train_counts = count_images_in_directory(train_dir)\n",
    "val_counts = count_images_in_directory(val_dir)\n",
    "\n",
    "# Output the counts for each directory\n",
    "print(\"Training image counts by class:\")\n",
    "for class_name, count in train_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n",
    "\n",
    "print(\"\\nValidation image counts by class:\")\n",
    "for class_name, count in val_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
