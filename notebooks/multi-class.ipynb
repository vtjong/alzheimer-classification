{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn\n",
    "\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import argparse\n",
    "\"\"\" Training and hyperparameter search configurations \"\"\"\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Final')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed (default: 1)')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# Set random seed to reproduce results\n",
    "np.random.seed(args.seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL and saves it to a local path.\n",
    "    \"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def download_oasis1(base_dir=\"/Users/valenetjong/Downloads/\"):\n",
    "    base_url = \"https://download.nrg.wustl.edu/data/oasis_cross-sectional_disc\"\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        url = f\"{base_url}{i}.tar.gz\"\n",
    "        local_filename = f\"oasis_cross-sectional_disc{i}.tar.gz\"\n",
    "        full_file_path = os.path.join(base_dir, local_filename)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(full_file_path):\n",
    "            print(f\"File {local_filename} already exists. Skipping download.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {url}\")\n",
    "        \n",
    "        try:\n",
    "            download_file(url, full_file_path)\n",
    "            print(f\"Downloaded {local_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {local_filename}: {e}\")\n",
    "\n",
    "def extract_tar_gz(tar_path, extract_to_path):\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file to a specified directory.\n",
    "    \"\"\"\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to_path)\n",
    "        print(f\"Extracted {tar_path} to {extract_to_path}\")\n",
    "\n",
    "def extract_all_discs(base_disc_path=\"/Users/valenetjong/Downloads/\", \n",
    "                    extract_to_path=\"/Users/valenetjong/Downloads/\"):\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        if os.path.exists(extract_to_path + f\"/disc{i}\") and os.path.isdir(extract_to_path + f\"/disc{i}\"):\n",
    "            print(f\"Folder for disc{i} already exists. Skipping extraction.\")\n",
    "            continue\n",
    "        tar_path = os.path.join(base_disc_path, f\"oasis_cross-sectional_disc{i}.tar.gz\")\n",
    "        os.makedirs(extract_to_path, exist_ok=True)\n",
    "        extract_tar_gz(tar_path, extract_to_path)\n",
    "\n",
    "        # Remove the tar.gz file after extraction\n",
    "        # os.remove(tar_path)\n",
    "        # print(f\"Removed the archive: {tar_path}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "download_oasis1()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File oasis_cross-sectional_disc1.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc2.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc3.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc4.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc5.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc6.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc7.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc8.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc9.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc10.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc11.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc12.tar.gz already exists. Skipping download.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "extract_all_discs()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folder for disc1 already exists. Skipping extraction.\n",
      "Folder for disc2 already exists. Skipping extraction.\n",
      "Folder for disc3 already exists. Skipping extraction.\n",
      "Folder for disc4 already exists. Skipping extraction.\n",
      "Folder for disc5 already exists. Skipping extraction.\n",
      "Folder for disc6 already exists. Skipping extraction.\n",
      "Folder for disc7 already exists. Skipping extraction.\n",
      "Folder for disc8 already exists. Skipping extraction.\n",
      "Folder for disc9 already exists. Skipping extraction.\n",
      "Folder for disc10 already exists. Skipping extraction.\n",
      "Folder for disc11 already exists. Skipping extraction.\n",
      "Folder for disc12 already exists. Skipping extraction.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import cv2 as cv\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\"\"\" Pre-processing Functions \"\"\"\n",
    "\n",
    "DEMENTIA_MAP = {\n",
    "    '0.0': \"nondemented\",\n",
    "    '0.5': \"mildly demented\",\n",
    "    '1.0': 'moderately demented',\n",
    "    '2.0': 'severely demented'\n",
    "}\n",
    "\n",
    "# Pre-determined max dimensions of cropped images\n",
    "CONV_WIDTH = 137\n",
    "CONV_HEIGHT = 167\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    \"\"\"\n",
    "    Normalizes the intensity of an image to the range [0, 255].\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    Normalized image.\n",
    "    \"\"\"\n",
    "    img_min = img.min()\n",
    "    img_max = img.max()\n",
    "    normalized_img = (img - img_min) / (img_max - img_min) * 255\n",
    "    return normalized_img.astype(np.uint8)\n",
    "\n",
    "def pad_image_to_size(img, width, height):\n",
    "    \"\"\"\n",
    "    Pads an image with zeros to the specified width and height.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be padded.\n",
    "    width: The desired width.\n",
    "    height: The desired height.\n",
    "\n",
    "    Returns:\n",
    "    Padded image.\n",
    "    \"\"\"\n",
    "    padded_img = np.zeros((height, width), dtype=img.dtype)\n",
    "    y_offset = (height - img.shape[0]) // 2\n",
    "    x_offset = (width - img.shape[1]) // 2\n",
    "    padded_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    return padded_img\n",
    "\n",
    "def crop_black_boundary(mri_image):\n",
    "    \"\"\"\n",
    "    Crops the black boundary from an MRI image.\n",
    "\n",
    "    Parameters:\n",
    "    mri_image: Input MRI image.\n",
    "\n",
    "    Returns:\n",
    "    Cropped MRI image with black boundaries removed.\n",
    "    \"\"\"\n",
    "    _, thresh = cv.threshold(mri_image, 1, 255, cv.THRESH_BINARY)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv.contourArea)\n",
    "    x, y, w, h = cv.boundingRect(largest_contour)\n",
    "    cropped_image = mri_image[y:y+h, x:x+w]\n",
    "    return cropped_image\n",
    "\n",
    "def extract_files(base_dir, target_dir, oasis_csv_path):\n",
    "    \"\"\"\n",
    "    Extracts and processes MRI files from a given directory.\n",
    "\n",
    "    Parameters:\n",
    "    base_dir: Directory containing MRI files.\n",
    "    target_dir: Directory where processed files will be saved.\n",
    "    oasis_csv_path: Path to the CSV file containing metadata.\n",
    "    \"\"\"\n",
    "    oasis_df = pd.read_csv(oasis_csv_path)\n",
    "\n",
    "    for subdir in filter(lambda d: d != '.DS_Store', os.listdir(base_dir)):\n",
    "        source_dir = os.path.join(base_dir, subdir, \"FSL_SEG\")\n",
    "        print(\"source_dir\", source_dir)\n",
    "        num = subdir.split('_')[1]\n",
    "        id = f'OAS1_{num}_MR1'\n",
    "        num = int(num)\n",
    "        row = oasis_df.loc[oasis_df['ID'] == id]\n",
    "        dementia_type = row['CDR'].item()\n",
    "        \n",
    "        if pd.isna(dementia_type):\n",
    "            continue\n",
    "\n",
    "        for n_suffix in ['n3', 'n4']:\n",
    "            fn = os.path.join(source_dir, f\"{subdir}_mpr_{n_suffix}_anon_\"\n",
    "                                  f\"111_t88_masked_gfc_fseg_tra_90.gif\")\n",
    "            if os.path.exists(fn):\n",
    "                process_image(fn, target_dir, dementia_type, id)\n",
    "\n",
    "def process_image(fn, target_dir, dementia_type, id):\n",
    "    \"\"\"\n",
    "    Processes a single MRI image file and saves it to the target directory.\n",
    "\n",
    "    Parameters:\n",
    "    fn: Path of the file to be processed.\n",
    "    target_dir: Directory where the processed file will be saved.\n",
    "    dementia_type: Type of dementia associated with the image.\n",
    "    id: Patient identifier associated with the image.\n",
    "    \"\"\"\n",
    "    with Image.open(fn) as img:\n",
    "        img = np.array(img.convert('RGB'))\n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    img = crop_black_boundary(img)\n",
    "    img = normalize_intensity(img)\n",
    "    img = pad_image_to_size(img, CONV_WIDTH, CONV_HEIGHT)\n",
    "\n",
    "    target_subdir = os.path.join(target_dir, DEMENTIA_MAP[str(dementia_type)])\n",
    "    os.makedirs(target_subdir, exist_ok=True)\n",
    "    target_path = os.path.join(target_subdir, f\"{id}.png\")\n",
    "    cv.imwrite(target_path, img)\n",
    "\n",
    "def process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path):\n",
    "    \"\"\"\n",
    "    Processes all discs found in the base directory.\n",
    "\n",
    "    Parameters:\n",
    "    base_disc_path: Base path where the discs are located.\n",
    "    base_extraction_path: Base path where processed data will be saved.\n",
    "    oasis_csv_path: Path to the OASIS CSV file.\n",
    "    \"\"\"\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        disc_path = f'{base_disc_path}/disc{i}'\n",
    "        if not os.path.exists(disc_path):\n",
    "            print(f\"Disc {i} does not exist at path {disc_path}. Skipping.\")\n",
    "            continue\n",
    "        extract_files(disc_path, base_extraction_path, oasis_csv_path)\n",
    "        print(f\"Processed Disc {i}\")\n",
    "\n",
    "        # Cleanup: delete the folder after processing\n",
    "        # cleanup_directory(disc_path)\n",
    "\n",
    "def cleanup_directory(path):\n",
    "    \"\"\"\n",
    "    Deletes a directory and all of its contents.\n",
    "\n",
    "    Parameters:\n",
    "    path: Path of the directory to be deleted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleaned up and deleted the directory: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e.filename} - {e.strerror}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "base_disc_path = '/Users/valenetjong/Downloads'\n",
    "base_extraction_path = '/Users/valenetjong/alzheimer-classification/data'\n",
    "oasis_csv_path = '/Users/valenetjong/alzheimer-classification/datacsv/oasis_cross-sectional.csv'\n",
    "\n",
    "process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0016_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0002_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0003_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0017_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0001_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0015_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0029_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0028_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0014_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0038_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0004_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0010_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0011_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0005_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0039_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0013_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0007_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0006_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0012_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0037_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0023_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0022_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0020_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0034_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0009_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0035_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0021_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0019_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0025_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0031_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0030_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0018_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0032_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0026_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0027_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0033_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0040_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0041_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc1/OAS1_0042_MR1/FSL_SEG\n",
      "Processed Disc 1\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0075_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0061_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0049_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0060_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0074_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0062_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0076_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0077_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0063_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0067_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0073_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0072_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0066_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0058_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0070_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0064_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0065_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0071_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0059_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0061_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0054_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0068_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0069_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0055_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0080_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0043_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0057_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0056_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0046_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0052_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0053_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0047_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0079_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0051_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0045_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0044_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0050_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc2/OAS1_0078_MR1/FSL_SEG\n",
      "Processed Disc 2\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0101_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0115_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0114_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0102_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0088_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0103_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0113_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0107_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0098_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0099_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0106_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0112_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0104_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0110_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0111_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0105_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0080_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0092_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0101_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0111_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0108_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0097_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0083_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0082_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0096_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0109_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0094_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0095_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0081_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0085_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0091_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0090_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0084_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0092_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0086_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc3/OAS1_0087_MR1/FSL_SEG\n",
      "Processed Disc 3\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0129_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0116_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0117_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0150_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0145_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0138_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0139_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0148_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0143_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0142_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0140_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0117_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0141_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0145_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0144_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0150_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0146_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0147_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0120_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0134_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0135_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0121_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0137_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0123_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0122_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0136_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0132_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0126_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0127_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0133_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0119_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0125_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0131_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0130_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc4/OAS1_0124_MR1/FSL_SEG\n",
      "Processed Disc 4\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0156_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0191_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0189_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0162_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0176_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0177_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0163_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0188_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0161_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0160_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0174_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0158_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0170_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0164_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0165_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0159_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0167_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0173_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0166_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0180_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0157_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0156_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0181_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0183_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0168_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0169_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0155_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0182_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0186_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0179_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0151_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0178_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0185_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0191_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0152_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0153_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0190_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc5/OAS1_0184_MR1/FSL_SEG\n",
      "Processed Disc 5\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0230_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0214_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0200_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0228_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0229_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0201_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0203_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0217_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0216_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0202_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0206_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0212_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0213_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0207_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0198_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0211_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0205_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0204_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0210_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0199_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0221_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0209_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0208_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0220_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0195_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0197_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0222_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0223_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0192_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0227_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0226_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0193_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0218_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0230_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0224_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0231_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc6/OAS1_0202_MR2/FSL_SEG\n",
      "Processed Disc 6\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0263_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0262_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0260_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0249_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0261_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0236_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0259_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0265_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0271_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0270_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0264_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0258_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0272_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0266_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0267_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0239_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0238_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0235_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0234_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0249_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0236_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0237_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0233_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0232_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0256_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0243_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0241_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0255_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0269_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0268_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0254_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0240_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0244_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0250_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0253_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0247_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0246_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc7/OAS1_0239_MR2/FSL_SEG\n",
      "Processed Disc 7\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0303_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0288_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0277_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0289_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0302_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0300_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0274_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0275_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0301_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0305_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0304_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0299_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0273_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0298_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0307_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0285_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0295_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0281_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0280_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0294_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0309_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0282_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0296_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0283_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0308_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0287_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0293_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0278_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0279_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0292_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0286_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0290_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0284_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0285_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc8/OAS1_0291_MR1/FSL_SEG\n",
      "Processed Disc 8\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0317_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0316_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0314_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0328_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0329_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0315_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0339_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0311_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0310_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0338_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0312_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0313_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0348_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0341_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0340_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0342_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0343_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0346_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0344_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0345_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0322_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0336_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0337_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0323_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0335_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0321_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0318_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0330_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0325_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0331_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0319_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0327_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0333_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0332_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc9/OAS1_0326_MR1/FSL_SEG\n",
      "Processed Disc 9\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0368_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0353_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0379_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0374_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0349_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0375_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0361_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0377_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0363_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0362_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0376_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0372_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0366_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0367_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0373_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0359_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0365_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0371_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0370_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0358_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0382_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0355_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0369_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0368_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0354_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0381_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0356_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0357_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0380_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0353_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0352_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0378_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0350_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0351_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc10/OAS1_0379_MR1/FSL_SEG\n",
      "Processed Disc 10\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0395_MR2/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0406_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0413_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0407_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0388_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0411_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0405_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0404_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0410_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0389_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0399_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0400_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0401_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0415_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0398_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0403_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0417_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0416_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0402_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0396_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0397_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0383_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0395_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0418_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0419_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0394_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0390_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0384_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0409_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0408_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0385_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0387_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0392_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc11/OAS1_0386_MR1/FSL_SEG\n",
      "Processed Disc 11\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0448_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0449_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0439_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0438_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0428_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0429_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0433_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0432_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0426_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0430_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0424_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0425_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0431_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0435_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0421_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0420_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0434_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0422_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0437_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0423_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0444_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0450_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0451_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0445_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0453_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0447_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0446_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0452_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0456_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0442_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0443_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0457_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0441_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0455_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0454_MR1/FSL_SEG\n",
      "source_dir /Users/valenetjong/Downloads/disc12/OAS1_0440_MR1/FSL_SEG\n",
      "Processed Disc 12\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"nondemented\": 0,\n",
    "    \"mildly demented\": 1,\n",
    "    'moderately demented': 2,\n",
    "    'severely demented' : 3\n",
    "}\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    class_counts = Counter()\n",
    "\n",
    "    # Automatically find all subdirectories in base_dir\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "            class_label = LABEL_MAP[folder_name]\n",
    "            for image_file in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                if os.path.isfile(image_path):\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img_tensor = transform(img)\n",
    "                        all_images.append(img_tensor)\n",
    "                        all_labels.append(class_label)\n",
    "                        class_counts[folder_name] += 1\n",
    "\n",
    "    X = torch.stack(all_images)\n",
    "    y = torch.tensor(all_labels, dtype=torch.long)  # Changed to long for integer labels\n",
    "    return X, y, class_counts\n",
    "\n",
    "base_dir = '/Users/valenetjong/alzheimer-classification/data'\n",
    "X, y, class_counts = load_dataset(base_dir)\n",
    "\n",
    "print(f\"Combined Tensor Size: {X.size()}\")\n",
    "print(f\"Labels Tensor Size: {y.size()}\")\n",
    "print(f\"Class Counts: {class_counts}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Combined Tensor Size: torch.Size([233, 1, 167, 137])\n",
      "Labels Tensor Size: torch.Size([233])\n",
      "Class Counts: Counter({'nondemented': 133, 'mildly demented': 70, 'moderately demented': 28, 'severely demented': 2})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_split(X, y, test_size=0.15, random_state=42, stratified=True):\n",
    "    # Convert X and y to numpy arrays if they are torch tensors\n",
    "    X_np = X.numpy() if isinstance(X, torch.Tensor) else X\n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "\n",
    "    # Stratified split\n",
    "    if stratified:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_np, y_np, test_size=test_size, random_state=random_state, stratify=y_np\n",
    "        )\n",
    "    # Random split\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_np, y_np, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    # Convert numpy arrays back to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    return X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_val_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Validation set size: {X_val.shape[0]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set size: 186\n",
      "Validation set size: 47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f\"Number of nondemented in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of mildly demented in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of moderately demented in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of severely demented in train dataset as percentage: {((y_train == 3).sum() / (X_train.shape[0])) * 100:0.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nondemented in train dataset as percentage: 56.99%\n",
      "Number of mildly demented in train dataset as percentage: 43.01%\n",
      "Number of moderately demented in train dataset as percentage: 0.00%\n",
      "Number of severely demented in train dataset as percentage: 0.00%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(f\"Number of nondemented in train dataset as percentage: {((y_val == 0).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of mildly demented in train dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of moderately demented in train dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of severely demented in train dataset as percentage: {((y_val == 3).sum() / (X_val.shape[0])) * 100:0.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nondemented in train dataset as percentage: 57.45%\n",
      "Number of mildly demented in train dataset as percentage: 42.55%\n",
      "Number of moderately demented in train dataset as percentage: 0.00%\n",
      "Number of severely demented in train dataset as percentage: 0.00%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "\"\"\" Transforms w/ probability \"\"\"\n",
    "def custom_random_rotation(image, probability=0.25, degrees=20):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomRotation(degrees=degrees)(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_resized_crop(image, probability=0.25, size=(CONV_HEIGHT, CONV_WIDTH), scale=(0.9, 1.0)):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomResizedCrop(size=size, scale=scale)(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_horizontal_flip(image, probability=0.25):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomHorizontalFlip()(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_affine(image, probability=0.25, translate=(0.1, 0.1), scale=None, shear=10):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomAffine(degrees=0, translate=translate, scale=scale, shear=shear)(image)\n",
    "    return image\n",
    "\n",
    "def apply_transforms(X):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = custom_random_rotation(x)\n",
    "        x = custom_random_resized_crop(x)\n",
    "        x = custom_random_horizontal_flip(x)\n",
    "        x = custom_random_affine(x)\n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "\n",
    "def apply_all_transforms(X, transform):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = transform(x)  # Apply the transformation\n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "\n",
    "\"\"\" Apply all transforms \"\"\"\n",
    "all_train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomResizedCrop(size=(CONV_HEIGHT, CONV_WIDTH), scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2), # You can adjust the values for brightness and contrast/\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=None, shear=10),\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_train_transformed = apply_all_transforms(X_train, all_train_transform)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/valenetjong/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Disproportionate Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    # Count the frequency of each class\n",
    "    class_counts = Counter(y_train.numpy())\n",
    "    total_samples = sum(class_counts.values())\n",
    "\n",
    "    # Calculate weights: Inverse of frequency\n",
    "    weights = {class_id: total_samples/class_counts[class_id] for class_id in class_counts}\n",
    "\n",
    "    # Convert to a list in the order of class ids\n",
    "    weights_list = [weights[i] for i in sorted(weights)]\n",
    "    return torch.tensor(weights_list, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define CNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)  \n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 4\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        # Compute the flattened size for the fully connected layer\n",
    "        self._to_linear = None\n",
    "        self._forward_conv(torch.randn(1, 1, 137, 167))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten the output for the fully connected layers\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', num_classes=4):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_classes)\n",
    "        else:\n",
    "            if isinstance(alpha, (float, int)):\n",
    "                self.alpha = torch.ones(num_classes) * alpha\n",
    "            else:\n",
    "                self.alpha = torch.tensor(alpha)\n",
    "        self.alpha = self.alpha / self.alpha.sum()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert targets to one-hot\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).to(inputs.device)\n",
    "\n",
    "        # Compute the log softmax\n",
    "        log_softmax = F.log_softmax(inputs, dim=1)\n",
    "\n",
    "        # Compute the loss per class\n",
    "        loss_per_class = -targets_one_hot * log_softmax\n",
    "\n",
    "        # Compute the focal loss factors\n",
    "        softmax_probs = torch.exp(log_softmax)\n",
    "        focal_factors = (1 - softmax_probs) ** self.gamma\n",
    "\n",
    "        # Apply alpha weighting and focal factors\n",
    "        alpha_factors = self.alpha.to(inputs.device).unsqueeze(0)\n",
    "        loss = alpha_factors * focal_factors * loss_per_class\n",
    "\n",
    "        # Sum over classes and compute the final loss based on reduction\n",
    "        loss = loss.sum(dim=1)\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = CNNModel(num_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# loss_function = FocalLoss(alpha=class_weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(X_train_transformed, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(num_epochs, model, optimizer, loss_function, train_loader, val_loader, stop_acc=70):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                \n",
    "            acc = 100 * correct / total\n",
    "            print(f'Validation Accuracy: {acc}%')\n",
    "            if acc >= stop_acc:\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Run training\n",
    "num_epochs = 500  # Set the number of epochs\n",
    "train_model(num_epochs, model, loss_function, train_loader, val_loader, stop_acc=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500, Loss: 1.033333996931712\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 2/500, Loss: 0.875230054060618\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 3/500, Loss: 0.7825907071431478\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 4/500, Loss: 0.7260761062304179\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 5/500, Loss: 0.724310298760732\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 6/500, Loss: 0.6542031168937683\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 7/500, Loss: 0.6741713881492615\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 8/500, Loss: 0.6540700395901998\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 9/500, Loss: 0.6517113645871481\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 10/500, Loss: 0.6654268503189087\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 11/500, Loss: 0.622650663057963\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 12/500, Loss: 0.6151897112528483\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 13/500, Loss: 0.6066718101501465\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 14/500, Loss: 0.6012760003407797\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 15/500, Loss: 0.6083651383717855\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 16/500, Loss: 0.5461878577868143\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 17/500, Loss: 0.5373141368230184\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 18/500, Loss: 0.5195347269376119\n",
      "Validation Accuracy: 55.319148936170215%\n",
      "Epoch 19/500, Loss: 0.5262607534726461\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 20/500, Loss: 0.5511021614074707\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 21/500, Loss: 0.5985181927680969\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 22/500, Loss: 0.5135003626346588\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 23/500, Loss: 0.481426735719045\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 24/500, Loss: 0.4777175386746724\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 25/500, Loss: 0.4520883560180664\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 26/500, Loss: 0.4173840284347534\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 27/500, Loss: 0.3777627448240916\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 28/500, Loss: 0.405078262090683\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 29/500, Loss: 0.37858515977859497\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 30/500, Loss: 0.34268417954444885\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 31/500, Loss: 0.2918580522139867\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 32/500, Loss: 0.2860059638818105\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 33/500, Loss: 0.28555862108866376\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 34/500, Loss: 0.3024590214093526\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 35/500, Loss: 0.2583411435286204\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 36/500, Loss: 0.2730943063894908\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 37/500, Loss: 0.27714910606543225\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 38/500, Loss: 0.23005993167559305\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 39/500, Loss: 0.2487959067026774\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 40/500, Loss: 0.2594428211450577\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 41/500, Loss: 0.2600694050391515\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 42/500, Loss: 0.24758229653040567\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 43/500, Loss: 0.20560699701309204\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 44/500, Loss: 0.18878291547298431\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 45/500, Loss: 0.2260409245888392\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 46/500, Loss: 0.1560597444574038\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 47/500, Loss: 0.19109723965326944\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 48/500, Loss: 0.19794684648513794\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 49/500, Loss: 0.1944593439499537\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 50/500, Loss: 0.1875379035870234\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 51/500, Loss: 0.216297576824824\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 52/500, Loss: 0.190562441945076\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 53/500, Loss: 0.22971321642398834\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 54/500, Loss: 0.18768852949142456\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 55/500, Loss: 0.17623378336429596\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 56/500, Loss: 0.17297528187433878\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 57/500, Loss: 0.19682962199052176\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 58/500, Loss: 0.19373656312624613\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 59/500, Loss: 0.21072622140248617\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 60/500, Loss: 0.24142451584339142\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 61/500, Loss: 0.18240371346473694\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 62/500, Loss: 0.1873618463675181\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 63/500, Loss: 0.1786730686823527\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 64/500, Loss: 0.20084468523661295\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 65/500, Loss: 0.22322588165601095\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 66/500, Loss: 0.17931742469469705\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 67/500, Loss: 0.21126429736614227\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 68/500, Loss: 0.14259755114714304\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 69/500, Loss: 0.16354234516620636\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 70/500, Loss: 0.17847459514935812\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 71/500, Loss: 0.1763968219359716\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 72/500, Loss: 0.2097704162200292\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 73/500, Loss: 0.1957601656516393\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 74/500, Loss: 0.20273153483867645\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 75/500, Loss: 0.1803970734278361\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 76/500, Loss: 0.19944790999094644\n",
      "Validation Accuracy: 46.808510638297875%\n",
      "Epoch 77/500, Loss: 0.16487972189982733\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 78/500, Loss: 0.19320728381474814\n",
      "Validation Accuracy: 82.97872340425532%\n",
      "Epoch 79/500, Loss: 0.19006320337454477\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 80/500, Loss: 0.20286126931508383\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 81/500, Loss: 0.16726797074079514\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 82/500, Loss: 0.18937950829664865\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 83/500, Loss: 0.1912380407253901\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 84/500, Loss: 0.1933434158563614\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 85/500, Loss: 0.16020385921001434\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 86/500, Loss: 0.22777666648228964\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 87/500, Loss: 0.20051085452238718\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 88/500, Loss: 0.1831701248884201\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 89/500, Loss: 0.18572330474853516\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 90/500, Loss: 0.18326384325822195\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 91/500, Loss: 0.17247694234053293\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 92/500, Loss: 0.2032064894835154\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 93/500, Loss: 0.19678604106108347\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 94/500, Loss: 0.17686828474203745\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 95/500, Loss: 0.1839874138434728\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 96/500, Loss: 0.17085175712903342\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 97/500, Loss: 0.1840595304965973\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 98/500, Loss: 0.1840344394246737\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 99/500, Loss: 0.1811339631676674\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 100/500, Loss: 0.16842293739318848\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 101/500, Loss: 0.1708431988954544\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 102/500, Loss: 0.180386483669281\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 103/500, Loss: 0.1905976931254069\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 104/500, Loss: 0.202110156416893\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 105/500, Loss: 0.16715559860070547\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 106/500, Loss: 0.18708698948224387\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 107/500, Loss: 0.16573530435562134\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 108/500, Loss: 0.2080395668745041\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 109/500, Loss: 0.18603683014710745\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 110/500, Loss: 0.16427339613437653\n",
      "Validation Accuracy: 68.08510638297872%\n",
      "Epoch 111/500, Loss: 0.15186179180939993\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 112/500, Loss: 0.18194645643234253\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 113/500, Loss: 0.17119056483109793\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 114/500, Loss: 0.14332529654105505\n",
      "Validation Accuracy: 68.08510638297872%\n",
      "Epoch 115/500, Loss: 0.19659468034903207\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 116/500, Loss: 0.1826158364613851\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 117/500, Loss: 0.18932761748631796\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 118/500, Loss: 0.22688385844230652\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 119/500, Loss: 0.16263273358345032\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 120/500, Loss: 0.16830583910147348\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 121/500, Loss: 0.16777828832467398\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 122/500, Loss: 0.17838449776172638\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 123/500, Loss: 0.1433957169453303\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 124/500, Loss: 0.2020538200934728\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 125/500, Loss: 0.15595493217309317\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 126/500, Loss: 0.19447407126426697\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 127/500, Loss: 0.16636365155378977\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 128/500, Loss: 0.19988954067230225\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 129/500, Loss: 0.18026791016260782\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 130/500, Loss: 0.17182384928067526\n",
      "Validation Accuracy: 42.5531914893617%\n",
      "Epoch 131/500, Loss: 0.1641759822765986\n",
      "Validation Accuracy: 82.97872340425532%\n",
      "Epoch 132/500, Loss: 0.18377078076203665\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 133/500, Loss: 0.16133926808834076\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 134/500, Loss: 0.1881794035434723\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 135/500, Loss: 0.17085948089758554\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 136/500, Loss: 0.14969487488269806\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 137/500, Loss: 0.22469337781270346\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 138/500, Loss: 0.18450337648391724\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 139/500, Loss: 0.19469602902730307\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 140/500, Loss: 0.2243654876947403\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 141/500, Loss: 0.1563963989416758\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 142/500, Loss: 0.20405339201291403\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 143/500, Loss: 0.1376175731420517\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 144/500, Loss: 0.177765225370725\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 145/500, Loss: 0.15584640701611838\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 146/500, Loss: 0.12529242038726807\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 147/500, Loss: 0.19241831203301749\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 148/500, Loss: 0.14475240310033163\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 149/500, Loss: 0.189467320839564\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 150/500, Loss: 0.1553335984547933\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 151/500, Loss: 0.1585861494143804\n",
      "Validation Accuracy: 70.2127659574468%\n",
      "Epoch 152/500, Loss: 0.20640993614991507\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 153/500, Loss: 0.20913947621981302\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 154/500, Loss: 0.16563727458318075\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 155/500, Loss: 0.11877553164958954\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 156/500, Loss: 0.1853760083516439\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 157/500, Loss: 0.19085917870203653\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 158/500, Loss: 0.19327556590239206\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 159/500, Loss: 0.19573984543482462\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 160/500, Loss: 0.17385580142339072\n",
      "Validation Accuracy: 68.08510638297872%\n",
      "Epoch 161/500, Loss: 0.17949528992176056\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 162/500, Loss: 0.1462260584036509\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 163/500, Loss: 0.1936675806840261\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 164/500, Loss: 0.2204758276542028\n",
      "Validation Accuracy: 57.4468085106383%\n",
      "Epoch 165/500, Loss: 0.19006914893786112\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 166/500, Loss: 0.17782561977704367\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 167/500, Loss: 0.16733554005622864\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 168/500, Loss: 0.15636825561523438\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 169/500, Loss: 0.2027809073527654\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 170/500, Loss: 0.18421389162540436\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 171/500, Loss: 0.21148012081782022\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 172/500, Loss: 0.1330037315686544\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 173/500, Loss: 0.18554076552391052\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 174/500, Loss: 0.18311254431804022\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 175/500, Loss: 0.15441668033599854\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 176/500, Loss: 0.18587822218736014\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 177/500, Loss: 0.16251120467980704\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 178/500, Loss: 0.1965449551741282\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 179/500, Loss: 0.17472816507021585\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 180/500, Loss: 0.18683024744192758\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 181/500, Loss: 0.1883865694204966\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 182/500, Loss: 0.13818119217952093\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 183/500, Loss: 0.17728106677532196\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 184/500, Loss: 0.1438229704896609\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 185/500, Loss: 0.18890288472175598\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 186/500, Loss: 0.2084229439496994\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 187/500, Loss: 0.17562341193358103\n",
      "Validation Accuracy: 68.08510638297872%\n",
      "Epoch 188/500, Loss: 0.157664492726326\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 189/500, Loss: 0.19481026629606882\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 190/500, Loss: 0.17370042204856873\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 191/500, Loss: 0.15565544366836548\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 192/500, Loss: 0.15879429380098978\n",
      "Validation Accuracy: 65.95744680851064%\n",
      "Epoch 193/500, Loss: 0.17649651070435843\n",
      "Validation Accuracy: 76.59574468085107%\n",
      "Epoch 194/500, Loss: 0.15443924566109976\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 195/500, Loss: 0.1633291890223821\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 196/500, Loss: 0.16083935896555582\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 197/500, Loss: 0.15777823080619177\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 198/500, Loss: 0.2174967626730601\n",
      "Validation Accuracy: 80.85106382978724%\n",
      "Epoch 199/500, Loss: 0.15614182005325952\n",
      "Validation Accuracy: 78.72340425531915%\n",
      "Epoch 200/500, Loss: 0.16612532238165537\n",
      "Validation Accuracy: 74.46808510638297%\n",
      "Epoch 201/500, Loss: 0.15226471424102783\n",
      "Validation Accuracy: 63.829787234042556%\n",
      "Epoch 202/500, Loss: 0.15986678004264832\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 203/500, Loss: 0.18937881787618002\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 204/500, Loss: 0.16832553843657175\n",
      "Validation Accuracy: 59.57446808510638%\n",
      "Epoch 205/500, Loss: 0.17539494733015695\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 206/500, Loss: 0.16292515397071838\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 207/500, Loss: 0.16763201355934143\n",
      "Validation Accuracy: 61.702127659574465%\n",
      "Epoch 208/500, Loss: 0.14731498559316\n",
      "Validation Accuracy: 72.34042553191489%\n",
      "Epoch 209/500, Loss: 0.17042455077171326\n",
      "Validation Accuracy: 78.72340425531915%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# Set the number of epochs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_acc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, model, loss_function, train_loader, val_loader, stop_acc)\u001b[0m\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, y_batch)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naive CNN performance achieves ~70% validation accuracy. We stop early when the validation accuracy is achieved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Next Steps\n",
    "- Explore different ConvNet architectures\n",
    "- Figure out why number of samples is so much less than actual number\n",
    "- Figure out how to deal with the inconsistent classes\n",
    "- Try ResNet (PyTorch has models)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)  # For ResNet18\n",
    "# resnet50 = models.resnet50(pretrained=True)  # For ResNet50"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/valenetjong/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/valenetjong/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class GrayscaleToRGBTransform:\n",
    "    def __call__(self, tensor):\n",
    "        # Check if the tensor has one channel (grayscale)\n",
    "        if tensor.shape[0] == 1:\n",
    "            # Repeat the tensor across 3 channels\n",
    "            tensor = tensor.repeat(3, 1, 1)\n",
    "        return tensor\n",
    "\n",
    "res_transform = transforms.Compose([\n",
    "    GrayscaleToRGBTransform(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def apply_all_transforms(X, transform):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = transform(x)  # Apply the transformation\n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "    \n",
    "X_train_resnet = apply_all_transforms(X_train, transform=res_transform)\n",
    "train_resnet = TensorDataset(X_train_resnet, y_train)\n",
    "trainloader_resnet = DataLoader(train_resnet, batch_size=32, shuffle=True)\n",
    "\n",
    "X_val_resnet = apply_all_transforms(X_val, transform=res_transform)\n",
    "val_resnet = TensorDataset(X_val_resnet, y_val)\n",
    "valloader_resnet = DataLoader(val_resnet, batch_size=32, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "num_epochs = 100  # Set the number of epochs\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = torch.nn.Linear(num_ftrs, 4) \n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(class_weights)\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)\n",
    "train_model(num_epochs, resnet18, criterion, optimizer, trainloader_resnet, valloader_resnet, stop_acc=70)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all 4 classes or no classes but got weight tensor of shape: [2]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(class_weights)\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(resnet18\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_acc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, model, loss_function, train_loader, val_loader, stop_acc)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: weight tensor should be defined either for all 4 classes or no classes but got weight tensor of shape: [2]"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.4 64-bit ('nlp-m': conda)"
  },
  "interpreter": {
   "hash": "62ef8ab7a50fde946441678ef251eba85852e5ab7813d8beba261a97f4cf7750"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}