{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\n",
    "import os, sys, re\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn\n",
    "\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvalenetjong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import argparse\n",
    "\"\"\" Training and hyperparameter search configurations \"\"\"\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Final')\n",
    "parser.add_argument('--img_dir', type=str, default='/Users/valenetjong/alzheimer-classification/data',\n",
    "                    help='directory for image storage')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--num_classes', type=int, default=3,\n",
    "                    help='number of classes')\n",
    "parser.add_argument('--loss', type=str, default=\"cross entropy\",\n",
    "                    help='cross entropy, focal')\n",
    "parser.add_argument('--process_flag', type=bool, default=False,\n",
    "                    help=\"extract files from disk if True, use already extracted files, if False\")\n",
    "parser.add_argument('--transforms', type=str, default='all',\n",
    "                    help='transforms for data augmentation')\n",
    "args = parser.parse_args('')\n",
    "# Set random seed to reproduce results\n",
    "np.random.seed(args.seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\" Set-up wandb \"\"\"\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'max val acc',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "params = {\n",
    "    'max_epochs': {\n",
    "        'value': 250\n",
    "        },\n",
    "    'hidden_size': {\n",
    "        'values': [8, 16],\n",
    "        },\n",
    "    'fc_size': {\n",
    "        'values': [32, 64, 128, 256, 512]\n",
    "        },\n",
    "    'conv_in_size': {\n",
    "        'values': [32, 64, 128, 256]\n",
    "        },\n",
    "    'conv_hid_size': {\n",
    "        'values': [8, 16, 32]\n",
    "        },\n",
    "    'conv_out_size': {\n",
    "        'values': [8, 16, 32]\n",
    "        },\n",
    "    'dropout': {\n",
    "          'values': [0.15, 0.2, 0.25, 0.3]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 8,\n",
    "        'max': 64,\n",
    "        },\n",
    "    'lr': {\n",
    "        'values': [1e-2, 1e-3, 1e-4, 1e-5]},\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = params\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"2D-masked-imgs\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create sweep with ID: qs2p82s9\n",
      "Sweep URL: https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL and saves it to a local path.\n",
    "    \"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def download_oasis1(base_dir=\"/Users/valenetjong/Downloads/\"):\n",
    "    base_url = \"https://download.nrg.wustl.edu/data/oasis_cross-sectional_disc\"\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        url = f\"{base_url}{i}.tar.gz\"\n",
    "        local_filename = f\"oasis_cross-sectional_disc{i}.tar.gz\"\n",
    "        full_file_path = os.path.join(base_dir, local_filename)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(full_file_path):\n",
    "            print(f\"File {local_filename} already exists. Skipping download.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {url}\")\n",
    "        \n",
    "        try:\n",
    "            download_file(url, full_file_path)\n",
    "            print(f\"Downloaded {local_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {local_filename}: {e}\")\n",
    "\n",
    "def extract_tar_gz(tar_path, extract_to_path):\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file to a specified directory.\n",
    "    \"\"\"\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to_path)\n",
    "        print(f\"Extracted {tar_path} to {extract_to_path}\")\n",
    "\n",
    "def extract_all_discs(base_disc_path=\"/Users/valenetjong/Downloads/\", \n",
    "                    extract_to_path=\"/Users/valenetjong/Downloads/\"):\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        if os.path.exists(extract_to_path + f\"/disc{i}\") and os.path.isdir(extract_to_path + f\"/disc{i}\"):\n",
    "            print(f\"Folder for disc{i} already exists. Skipping extraction.\")\n",
    "            continue\n",
    "        tar_path = os.path.join(base_disc_path, f\"oasis_cross-sectional_disc{i}.tar.gz\")\n",
    "        os.makedirs(extract_to_path, exist_ok=True)\n",
    "        extract_tar_gz(tar_path, extract_to_path)\n",
    "\n",
    "        # Remove the tar.gz file after extraction\n",
    "        # os.remove(tar_path)\n",
    "        # print(f\"Removed the archive: {tar_path}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "download_oasis1()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File oasis_cross-sectional_disc1.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc2.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc3.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc4.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc5.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc6.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc7.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc8.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc9.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc10.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc11.tar.gz already exists. Skipping download.\n",
      "File oasis_cross-sectional_disc12.tar.gz already exists. Skipping download.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "extract_all_discs()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folder for disc1 already exists. Skipping extraction.\n",
      "Folder for disc2 already exists. Skipping extraction.\n",
      "Folder for disc3 already exists. Skipping extraction.\n",
      "Folder for disc4 already exists. Skipping extraction.\n",
      "Folder for disc5 already exists. Skipping extraction.\n",
      "Folder for disc6 already exists. Skipping extraction.\n",
      "Folder for disc7 already exists. Skipping extraction.\n",
      "Folder for disc8 already exists. Skipping extraction.\n",
      "Folder for disc9 already exists. Skipping extraction.\n",
      "Folder for disc10 already exists. Skipping extraction.\n",
      "Folder for disc11 already exists. Skipping extraction.\n",
      "Folder for disc12 already exists. Skipping extraction.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import cv2 as cv\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\"\"\" Pre-processing Functions \"\"\"\n",
    "\n",
    "DEMENTIA_MAP = {\n",
    "    '0.0': \"nondemented\",\n",
    "    '0.5': \"mildly demented\",\n",
    "    '1.0': 'moderately demented',\n",
    "}\n",
    "\n",
    "# Pre-determined max dimensions of cropped images\n",
    "CONV_WIDTH = 137\n",
    "CONV_HEIGHT = 167\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    \"\"\"\n",
    "    Normalizes the intensity of an image to the range [0, 255].\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    Normalized image.\n",
    "    \"\"\"\n",
    "    img_min = img.min()\n",
    "    img_max = img.max()\n",
    "    normalized_img = (img - img_min) / (img_max - img_min) * 255\n",
    "    return normalized_img.astype(np.uint8)\n",
    "\n",
    "def pad_image_to_size(img, width, height):\n",
    "    \"\"\"\n",
    "    Pads an image with zeros to the specified width and height.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be padded.\n",
    "    width: The desired width.\n",
    "    height: The desired height.\n",
    "\n",
    "    Returns:\n",
    "    Padded image.\n",
    "    \"\"\"\n",
    "    padded_img = np.zeros((height, width), dtype=img.dtype)\n",
    "    y_offset = (height - img.shape[0]) // 2\n",
    "    x_offset = (width - img.shape[1]) // 2\n",
    "    padded_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    return padded_img\n",
    "\n",
    "def crop_black_boundary(mri_image):\n",
    "    \"\"\"\n",
    "    Crops the black boundary from an MRI image.\n",
    "\n",
    "    Parameters:\n",
    "    mri_image: Input MRI image.\n",
    "\n",
    "    Returns:\n",
    "    Cropped MRI image with black boundaries removed.\n",
    "    \"\"\"\n",
    "    _, thresh = cv.threshold(mri_image, 1, 255, cv.THRESH_BINARY)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv.contourArea)\n",
    "    x, y, w, h = cv.boundingRect(largest_contour)\n",
    "    cropped_image = mri_image[y:y+h, x:x+w]\n",
    "    return cropped_image\n",
    "\n",
    "def extract_files(base_dir, target_dir, oasis_csv_path):\n",
    "    \"\"\"\n",
    "    Extracts and processes MRI files from a given directory.\n",
    "\n",
    "    Parameters:\n",
    "    base_dir: Directory containing MRI files.\n",
    "    target_dir: Directory where processed files will be saved.\n",
    "    oasis_csv_path: Path to the CSV file containing metadata.\n",
    "    \"\"\"\n",
    "    oasis_df = pd.read_csv(oasis_csv_path)\n",
    "\n",
    "    for subdir in filter(lambda d: d != '.DS_Store', os.listdir(base_dir)):\n",
    "        source_dir = os.path.join(base_dir, subdir, \"FSL_SEG\")\n",
    "        print(\"source_dir\", source_dir)\n",
    "        num = subdir.split('_')[1]\n",
    "        id = f'OAS1_{num}_MR1'\n",
    "        num = int(num)\n",
    "        row = oasis_df.loc[oasis_df['ID'] == id]\n",
    "        dementia_type = row['CDR'].item()\n",
    "        \n",
    "        if pd.isna(dementia_type):\n",
    "            continue\n",
    "\n",
    "        for n_suffix in ['n3', 'n4']:\n",
    "            fn = os.path.join(source_dir, f\"{subdir}_mpr_{n_suffix}_anon_\"\n",
    "                                  f\"111_t88_masked_gfc_fseg_tra_90.gif\")\n",
    "            if os.path.exists(fn):\n",
    "                process_image(fn, target_dir, dementia_type, id)\n",
    "\n",
    "def process_image(fn, target_dir, dementia_type, id):\n",
    "    \"\"\"\n",
    "    Processes a single MRI image file and saves it to the target directory.\n",
    "\n",
    "    Parameters:\n",
    "    fn: Path of the file to be processed.\n",
    "    target_dir: Directory where the processed file will be saved.\n",
    "    dementia_type: Type of dementia associated with the image.\n",
    "    id: Patient identifier associated with the image.\n",
    "    \"\"\"\n",
    "    with Image.open(fn) as img:\n",
    "        img = np.array(img.convert('RGB'))\n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    img = crop_black_boundary(img)\n",
    "    img = normalize_intensity(img)\n",
    "    img = pad_image_to_size(img, CONV_WIDTH, CONV_HEIGHT)\n",
    "\n",
    "    target_subdir = os.path.join(target_dir, DEMENTIA_MAP[str(dementia_type)])\n",
    "    os.makedirs(target_subdir, exist_ok=True)\n",
    "    target_path = os.path.join(target_subdir, f\"{id}.png\")\n",
    "    cv.imwrite(target_path, img)\n",
    "\n",
    "def process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path):\n",
    "    \"\"\"\n",
    "    Processes all discs found in the base directory.\n",
    "\n",
    "    Parameters:\n",
    "    base_disc_path: Base path where the discs are located.\n",
    "    base_extraction_path: Base path where processed data will be saved.\n",
    "    oasis_csv_path: Path to the OASIS CSV file.\n",
    "    \"\"\"\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        disc_path = f'{base_disc_path}/disc{i}'\n",
    "        if not os.path.exists(disc_path):\n",
    "            print(f\"Disc {i} does not exist at path {disc_path}. Skipping.\")\n",
    "            continue\n",
    "        extract_files(disc_path, base_extraction_path, oasis_csv_path)\n",
    "        print(f\"Processed Disc {i}\")\n",
    "\n",
    "def cleanup_directory(path):\n",
    "    \"\"\"\n",
    "    Deletes a directory and all of its contents.\n",
    "\n",
    "    Parameters:\n",
    "    path: Path of the directory to be deleted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleaned up and deleted the directory: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e.filename} - {e.strerror}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "base_disc_path = '/Users/valenetjong/Downloads'\n",
    "base_extraction_path = '/Users/valenetjong/alzheimer-classification/data'\n",
    "oasis_csv_path = '/Users/valenetjong/alzheimer-classification/datacsv/oasis_cross-sectional.csv'\n",
    "\n",
    "if args.process_flag:\n",
    "    process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"nondemented\": 0,\n",
    "    \"mildly demented\": 1,\n",
    "    'moderately demented': 1 if args.num_classes == 2 else 2\n",
    "}\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])    \n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    class_counts = Counter()\n",
    "\n",
    "    # Automatically find all subdirectories in base_dir\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "            class_label = LABEL_MAP[folder_name]\n",
    "            for image_file in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                if os.path.isfile(image_path):\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img_tensor = transform(img)\n",
    "                        all_images.append(img_tensor)\n",
    "                        all_labels.append(class_label)\n",
    "                        class_counts[folder_name] += 1\n",
    "\n",
    "    X = torch.stack(all_images)\n",
    "    y = torch.tensor(all_labels, dtype=torch.long)  # Changed to long for integer labels\n",
    "    return X, y, class_counts\n",
    "\n",
    "X, y, class_counts = load_dataset(args.img_dir)\n",
    "\n",
    "print(f\"Combined Tensor Size: {X.size()}\")\n",
    "print(f\"Labels Tensor Size: {y.size()}\")\n",
    "print(f\"Class Counts: {class_counts}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Combined Tensor Size: torch.Size([235, 1, 167, 137])\n",
      "Labels Tensor Size: torch.Size([235])\n",
      "Class Counts: Counter({'nondemented': 135, 'mildly demented': 70, 'moderately demented': 30})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_split(X, y, test_size=0.2, random_state=42, stratified=True):\n",
    "    # Convert X and y to numpy arrays if they are torch tensors\n",
    "    X_np = X.numpy() if isinstance(X, torch.Tensor) else X\n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "\n",
    "    # Stratified split\n",
    "    if stratified:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_np, y_np, test_size=test_size, random_state=random_state, stratify=y_np\n",
    "        )\n",
    "    # Random split\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_np, y_np, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    # Convert numpy arrays back to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    return X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_val_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Validation set size: {X_val.shape[0]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set size: 188\n",
      "Validation set size: 47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(f\"Number of nondemented in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of mildly demented in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of moderately demented in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nondemented in train dataset as percentage: 57.45%\n",
      "Number of mildly demented in train dataset as percentage: 29.79%\n",
      "Number of moderately demented in train dataset as percentage: 12.77%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(f\"Number of nondemented in train dataset as percentage: {((y_val == 0).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of mildly demented in train dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of moderately demented in train dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nondemented in train dataset as percentage: 57.45%\n",
      "Number of mildly demented in train dataset as percentage: 29.79%\n",
      "Number of moderately demented in train dataset as percentage: 12.77%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "\"\"\" Transforms w/ probability \"\"\"\n",
    "def custom_random_rotation(image, probability=0.25, min_degree=20, max_degree=40):\n",
    "    if random.random() < probability:\n",
    "        degrees = random.randint(min_degree, max_degree)\n",
    "        return transforms.RandomRotation(degrees=degrees)(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_resized_crop(image, probability=0.25, size=(CONV_HEIGHT, CONV_WIDTH), scale=(0.9, 1.0)):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomResizedCrop(size=size, scale=scale)(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_horizontal_flip(image, probability=0.25):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomHorizontalFlip()(image)\n",
    "    return image\n",
    "\n",
    "def custom_random_affine(image, probability=0.25, translate=(0.1, 0.1), scale=None, shear=10):\n",
    "    if random.random() < probability:\n",
    "        return transforms.RandomAffine(degrees=0, translate=translate, scale=scale, shear=shear)(image)\n",
    "    return image\n",
    "\n",
    "def custom_color_jitter(image, probability=0.25, brightness=0.2, contrast=0.2):\n",
    "    if random.random() < probability:\n",
    "        return transforms.ColorJitter(brightness=brightness, contrast=contrast)(image)\n",
    "    return image\n",
    "\n",
    "def apply_transforms(X):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = custom_random_rotation(x)\n",
    "        x = custom_random_resized_crop(x)\n",
    "        x = custom_random_horizontal_flip(x)\n",
    "        x = custom_random_affine(x)\n",
    "        x = custom_color_jitter(x)\n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "\n",
    "def apply_all_transforms(X, transform):\n",
    "    transformed_data = []\n",
    "    for x in X:\n",
    "        x = transform(x)  # Apply the transformation\n",
    "        transformed_data.append(x)\n",
    "    return torch.stack(transformed_data)\n",
    "\n",
    "all_train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomResizedCrop(size=(CONV_HEIGHT, CONV_WIDTH), scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2), # You can adjust the values for brightness and contrast/\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=None, shear=10),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(X_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([188, 1, 167, 137])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "num_augment = 1\n",
    "# augment_list += [apply_transforms(X_train) for _ in range(num_augment-1)]\n",
    "X_augmented = apply_all_transforms(X_train, all_train_transform)\n",
    "y_augmented = y_train\n",
    "\n",
    "print(X_augmented.shape)\n",
    "print(y_augmented.shape)\n",
    "\n",
    "torch.save(X_augmented, 'X_augmented.pt')\n",
    "torch.save(y_augmented, 'y_augmented.pt')\n",
    "torch.save(X_val, 'X_val.pt')\n",
    "torch.save(y_val, 'y_val.pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/valenetjong/opt/anaconda3/envs/nlp-m/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([188, 1, 167, 137])\n",
      "torch.Size([188])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Disproportionate Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    # Count the frequency of each class\n",
    "    class_counts = Counter(y_train.numpy())\n",
    "    total_samples = sum(class_counts.values())\n",
    "\n",
    "    # Calculate weights: Inverse of frequency\n",
    "    weights = {class_id: total_samples/class_counts[class_id] for class_id in class_counts}\n",
    "\n",
    "    # Convert to a list in the order of class ids\n",
    "    weights_list = [weights[i] for i in sorted(weights)]\n",
    "    return torch.tensor(weights_list, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define CNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add skip connections \n",
    "# Number of conv. features should be correlated to number of segments\n",
    "# Other transformation types ()\n",
    "\n",
    "class DeepCNNModel(nn.Module):\n",
    "    def __init__(self, fc_size, conv_in_size, conv_hid_size, conv_out_size, dropout, num_classes=3):\n",
    "        super(DeepCNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(1, conv_in_size, kernel_size=3, padding=1)  \n",
    "        self.bn1 = nn.BatchNorm2d(conv_in_size)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv2d(conv_in_size, conv_hid_size, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(conv_hid_size)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv2d(conv_hid_size, conv_hid_size, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(conv_hid_size)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 4\n",
    "        self.conv4 = nn.Conv2d(conv_hid_size, conv_out_size, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(conv_out_size)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        # Compute the flattened size for the fully connected layer\n",
    "        self._to_linear = None\n",
    "        self._forward_conv(torch.randn(1, 1, 137, 167))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, fc_size)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten the output for the fully connected layers\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', num_classes=args.num_classes):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_classes)\n",
    "        else:\n",
    "            if isinstance(alpha, (float, int)):\n",
    "                self.alpha = torch.ones(num_classes) * alpha\n",
    "            else:\n",
    "                self.alpha = torch.tensor(alpha)\n",
    "        self.alpha = self.alpha / self.alpha.sum()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert targets to one-hot\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).to(inputs.device)\n",
    "\n",
    "        # Compute the log softmax\n",
    "        log_softmax = F.log_softmax(inputs, dim=1)\n",
    "\n",
    "        # Compute the loss per class\n",
    "        loss_per_class = -targets_one_hot * log_softmax\n",
    "\n",
    "        # Compute the focal loss factors\n",
    "        softmax_probs = torch.exp(log_softmax)\n",
    "        focal_factors = (1 - softmax_probs) ** self.gamma\n",
    "\n",
    "        # Apply alpha weighting and focal factors\n",
    "        alpha_factors = self.alpha.to(inputs.device).unsqueeze(0)\n",
    "        loss = alpha_factors * focal_factors * loss_per_class\n",
    "\n",
    "        # Sum over classes and compute the final loss based on reduction\n",
    "        loss = loss.sum(dim=1)\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import logging\n",
    "\n",
    "class WandbModelCheckpoint:\n",
    "    def __init__(self, dirpath, decreasing=False, top_n=1):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights \n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n \n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        \n",
    "    def __call__(self, model, epoch, metric_val):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save: \n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}, & logging model weights to W&B.\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            self.log_artifact(f'model-ckpt-epoch-{epoch}.pt', model_path, metric_val)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n: \n",
    "            self.cleanup()\n",
    "    \n",
    "    def log_artifact(self, filename, model_path, metric_val):\n",
    "        artifact = wandb.Artifact(filename, type='model', metadata={'Validation score': metric_val})\n",
    "        artifact.add_file(model_path)\n",
    "        wandb.run.log_artifact(artifact)        \n",
    "    \n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights) if args.loss == 'cross entropy' else FocalLoss(alpha=class_weights)\n",
    "\n",
    "# Training Function\n",
    "def train_model(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        # wandb.define_metric(\"epoch\")\n",
    "        # wandb.define_metric(\"val acc\", step_metric=\"epoch\")\n",
    "        model = DeepCNNModel(config.fc_size, config.conv_in_size, config.conv_hid_size, config.conv_out_size, config.dropout, num_classes=3)\n",
    "        optimizer = optim.Adam(model.parameters(), config.lr, weight_decay=0.0001)   \n",
    "        batch_size = config.batch_size\n",
    "        \n",
    "        X_augmented = torch.load('X_augmented.pt')\n",
    "        y_augmented = torch.load('y_augmented.pt')\n",
    "        train_data = TensorDataset(X_augmented, y_augmented)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        X_val = torch.load('X_val.pt')\n",
    "        y_val = torch.load('y_val.pt')\n",
    "        val_data = TensorDataset(X_val, y_val)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "        \n",
    "        max_acc = 0\n",
    "        for epoch in range(config.max_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch)\n",
    "                loss = loss_function(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"batch loss\": loss.item()})\n",
    "            # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    output = model(X_batch)\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += y_batch.size(0)\n",
    "                    correct += (predicted == y_batch).sum().item()\n",
    "                    \n",
    "                acc = 100 * correct / total\n",
    "                \n",
    "                wandb.log({\"val acc\": acc})\n",
    "                if acc >= max_acc:\n",
    "                    max_acc = acc\n",
    "                    wandb.log({\"max val acc\": max_acc})\n",
    "                    checkpoint_dir(model, epoch, acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Run training\n",
    "checkpoint_dir = \"./model_checkpoints\"\n",
    "checkpoint = WandbModelCheckpoint(checkpoint_dir, decreasing=False, top_n=3)\n",
    "wandb.agent(sweep_id, train_model, count=50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4i9zb8bu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_hid_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_in_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_out_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 250\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/valenetjong/alzheimer-classification/notebooks/wandb/run-20231213_230147-4i9zb8bu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/valenetjong/2D-masked-imgs/runs/4i9zb8bu' target=\"_blank\">crimson-sweep-1</a></strong> to <a href='https://wandb.ai/valenetjong/2D-masked-imgs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/valenetjong/2D-masked-imgs' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/valenetjong/2D-masked-imgs/runs/4i9zb8bu' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/runs/4i9zb8bu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▄▇▃█▂▄▅▄▃▃▄▄▃▄▄▄▄▄▄▃▂▃▃▂▂▃▂▃▄▂▃▂▃▂▂▂▃▁▃</td></tr><tr><td>max val acc</td><td>▁▇██</td></tr><tr><td>val acc</td><td>█▅▂▁▁▂▁▂▂▁▁▁▂▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▂▃▂▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>1.02592</td></tr><tr><td>max val acc</td><td>57.44681</td></tr><tr><td>val acc</td><td>34.04255</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-sweep-1</strong> at: <a href='https://wandb.ai/valenetjong/2D-masked-imgs/runs/4i9zb8bu' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/runs/4i9zb8bu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231213_230147-4i9zb8bu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ftzkm6l0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_hid_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_in_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_out_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 250\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/valenetjong/alzheimer-classification/notebooks/wandb/run-20231213_230705-ftzkm6l0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/valenetjong/2D-masked-imgs/runs/ftzkm6l0' target=\"_blank\">avid-sweep-2</a></strong> to <a href='https://wandb.ai/valenetjong/2D-masked-imgs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/valenetjong/2D-masked-imgs' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/sweeps/qs2p82s9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/valenetjong/2D-masked-imgs/runs/ftzkm6l0' target=\"_blank\">https://wandb.ai/valenetjong/2D-masked-imgs/runs/ftzkm6l0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naive CNN performance achieves ~70% validation accuracy. We stop early when the validation accuracy is achieved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Next Steps\n",
    "- Explore different ConvNet architectures\n",
    "- Figure out why number of samples is so much less than actual number\n",
    "- Figure out how to deal with the inconsistent classes\n",
    "- Try ResNet (PyTorch has models)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import torch\n",
    "# import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import ImageFolder\n",
    "\n",
    "# resnet18 = models.resnet18(pretrained=True)  # For ResNet18\n",
    "# resnet50 = models.resnet50(pretrained=True)  # For ResNet50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# class GrayscaleToRGBTransform:\n",
    "#     def __call__(self, tensor):\n",
    "#         # Check if the tensor has one channel (grayscale)\n",
    "#         if tensor.shape[0] == 1:\n",
    "#             # Repeat the tensor across 3 channels\n",
    "#             tensor = tensor.repeat(3, 1, 1)\n",
    "#         return tensor\n",
    "\n",
    "# res_transform = transforms.Compose([\n",
    "#     GrayscaleToRGBTransform(),\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# def apply_all_transforms(X, transform):\n",
    "#     transformed_data = []\n",
    "#     for x in X:\n",
    "#         x = transform(x)  # Apply the transformation\n",
    "#         transformed_data.append(x)\n",
    "#     return torch.stack(transformed_data)\n",
    "    \n",
    "# X_train_resnet = apply_all_transforms(X_train, transform=res_transform)\n",
    "# train_resnet = TensorDataset(X_train_resnet, y_train)\n",
    "# trainloader_resnet = DataLoader(train_resnet, batch_size=32, shuffle=True)\n",
    "\n",
    "# X_val_resnet = apply_all_transforms(X_val, transform=res_transform)\n",
    "# val_resnet = TensorDataset(X_val_resnet, y_val)\n",
    "# valloader_resnet = DataLoader(val_resnet, batch_size=32, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# num_epochs = 100  # Set the number of epochs\n",
    "# num_ftrs = resnet18.fc.in_features\n",
    "# resnet18.fc = torch.nn.Linear(num_ftrs, args.num_classes) \n",
    "\n",
    "# # Define a loss function and optimizer\n",
    "# criterion = torch.nn.CrossEntropyLoss(class_weights)\n",
    "# optimizer = torch.optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)\n",
    "# train_model(num_epochs, resnet18, criterion, optimizer, trainloader_resnet, valloader_resnet, stop_acc=70)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.4 64-bit ('nlp-m': conda)"
  },
  "interpreter": {
   "hash": "62ef8ab7a50fde946441678ef251eba85852e5ab7813d8beba261a97f4cf7750"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}