{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn\n",
    "\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import cv2 as cv\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "\"\"\" Pre-processing Functions \"\"\"\n",
    "\n",
    "DEMENTIA_MAP = {\n",
    "    '0.0': \"nondemented\",\n",
    "    '0.5': \"mildly demented\",\n",
    "    '1.0': 'moderately demented',\n",
    "    '2.0': 'severely demented'\n",
    "}\n",
    "\n",
    "# Pre-determined max dimensions of cropped images\n",
    "CONV_WIDTH = 137\n",
    "CONV_HEIGHT = 167\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    \"\"\"\n",
    "    Normalizes the intensity of an image to the range [0, 255].\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    Normalized image.\n",
    "    \"\"\"\n",
    "    img_min = img.min()\n",
    "    img_max = img.max()\n",
    "    normalized_img = (img - img_min) / (img_max - img_min) * 255\n",
    "    return normalized_img.astype(np.uint8)\n",
    "\n",
    "def pad_image_to_size(img, width, height):\n",
    "    \"\"\"\n",
    "    Pads an image with zeros to the specified width and height.\n",
    "\n",
    "    Parameters:\n",
    "    img: The image to be padded.\n",
    "    width: The desired width.\n",
    "    height: The desired height.\n",
    "\n",
    "    Returns:\n",
    "    Padded image.\n",
    "    \"\"\"\n",
    "    if img.shape[0] > height or img.shape[1] > width:\n",
    "        scaling_factor = min(width / img.shape[1], height / img.shape[0])\n",
    "        img = cv.resize(img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv.INTER_AREA)\n",
    "\n",
    "    padded_img = np.zeros((height, width), dtype=img.dtype)\n",
    "    y_offset = (height - img.shape[0]) // 2\n",
    "    x_offset = (width - img.shape[1]) // 2\n",
    "    padded_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    return padded_img\n",
    "\n",
    "def crop_black_boundary(mri_image):\n",
    "    \"\"\"\n",
    "    Crops the black boundary from an MRI image.\n",
    "\n",
    "    Parameters:\n",
    "    mri_image: Input MRI image.\n",
    "\n",
    "    Returns:\n",
    "    Cropped MRI image with black boundaries removed.\n",
    "    \"\"\"\n",
    "    _, thresh = cv.threshold(mri_image, 1, 255, cv.THRESH_BINARY)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv.contourArea)\n",
    "    x, y, w, h = cv.boundingRect(largest_contour)\n",
    "    cropped_image = mri_image[y:y+h, x:x+w]\n",
    "    return cropped_image\n",
    "\n",
    "def extract_files(base_dir, target_dir, oasis_csv_path):\n",
    "    oasis_df = pd.read_csv(oasis_csv_path)\n",
    "\n",
    "    for subdir in filter(lambda d: d != '.DS_Store', os.listdir(base_dir)):\n",
    "        source_dir = os.path.join(base_dir, subdir, \"PROCESSED\", \"MPRAGE\", \"T88_111\")\n",
    "        identifier = subdir\n",
    "        if identifier not in oasis_df['ID'].values:\n",
    "            continue\n",
    "        row = oasis_df[oasis_df['ID'] == identifier]\n",
    "        dementia_type = row['CDR'].iloc[0]\n",
    "        if pd.isna(dementia_type):\n",
    "            continue\n",
    "\n",
    "        for n_suffix in ['n3', 'n4', 'n5']:\n",
    "            for img_type in ['tra', 'cor', 'sag']:\n",
    "                pattern = f\"{subdir}_mpr_{n_suffix}_anon_111_t88_gfc_{img_type}_*.gif\"\n",
    "                for file in os.listdir(source_dir):\n",
    "                    if fnmatch.fnmatch(file, pattern):\n",
    "                        fn = os.path.join(source_dir, file)\n",
    "                        process_image(fn, target_dir, dementia_type, identifier, img_type)\n",
    "                        break\n",
    "\n",
    "\n",
    "def process_image(fn, target_dir, dementia_type, id):\n",
    "    \"\"\"\n",
    "    Processes a single MRI image file and saves it to the target directory.\n",
    "\n",
    "    Parameters:\n",
    "    fn: Path of the file to be processed.\n",
    "    target_dir: Directory where the processed file will be saved.\n",
    "    dementia_type: Type of dementia associated with the image.\n",
    "    id: Patient identifier associated with the image.\n",
    "    \"\"\"\n",
    "    with Image.open(fn) as img:\n",
    "        img = np.array(img.convert('RGB'))\n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    img = crop_black_boundary(img)\n",
    "    img = normalize_intensity(img)\n",
    "    img = pad_image_to_size(img, CONV_WIDTH, CONV_HEIGHT)\n",
    "\n",
    "    target_subdir = os.path.join(target_dir, DEMENTIA_MAP[str(dementia_type)])\n",
    "    os.makedirs(target_subdir, exist_ok=True)\n",
    "    target_path = os.path.join(target_subdir, f\"{id}.png\")\n",
    "    cv.imwrite(target_path, img)\n",
    "\n",
    "def process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path):\n",
    "    \"\"\"\n",
    "    Processes all discs found in the base directory.\n",
    "\n",
    "    Parameters:\n",
    "    base_disc_path: Base path where the discs are located.\n",
    "    base_extraction_path: Base path where processed data will be saved.\n",
    "    oasis_csv_path: Path to the OASIS CSV file.\n",
    "    \"\"\"\n",
    "    total_disks = 12\n",
    "\n",
    "    for i in range(1, total_disks + 1):\n",
    "        disc_path = os.path.join(base_disc_path, f'disc{i}')\n",
    "        print(f\"Processing Disc {i} at path: {disc_path}\")\n",
    "\n",
    "        if not os.path.exists(disc_path):\n",
    "            print(f\"Disc {i} does not exist at path {disc_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        extract_files(disc_path, base_extraction_path, oasis_csv_path)\n",
    "        print(f\"Finished processing Disc {i}\")\n",
    "\n",
    "        # Cleanup: delete the folder after processing\n",
    "        # cleanup_directory(disc_path)\n",
    "\n",
    "def cleanup_directory(path):\n",
    "    \"\"\"\n",
    "    Deletes a directory and all of its contents.\n",
    "\n",
    "    Parameters:\n",
    "    path: Path of the directory to be deleted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleaned up and deleted the directory: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e.filename} - {e.strerror}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Disc 1 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc1\n",
      "Finished processing Disc 1\n",
      "Processing Disc 2 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc2\n",
      "Finished processing Disc 2\n",
      "Processing Disc 3 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc3\n",
      "Finished processing Disc 3\n",
      "Processing Disc 4 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc4\n",
      "Finished processing Disc 4\n",
      "Processing Disc 5 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc5\n",
      "Finished processing Disc 5\n",
      "Processing Disc 6 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc6\n",
      "Finished processing Disc 6\n",
      "Processing Disc 7 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc7\n",
      "Finished processing Disc 7\n",
      "Processing Disc 8 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc8\n",
      "Finished processing Disc 8\n",
      "Processing Disc 9 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc9\n",
      "Finished processing Disc 9\n",
      "Processing Disc 10 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc10\n",
      "Finished processing Disc 10\n",
      "Processing Disc 11 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc11\n",
      "Finished processing Disc 11\n",
      "Processing Disc 12 at path: /Users/msturman00/Documents/GitHub/alzheimer-classification/data2/disc12\n",
      "Finished processing Disc 12\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_path = '/Users/msturman00/Documents/GitHub/alzheimer-classification/'\n",
    "base_disc_path = base_path+'data2'\n",
    "base_extraction_path = './data'\n",
    "oasis_csv_path = base_path + './datacsv/oasis_cross-sectional.csv'\n",
    "process_all_discs(base_disc_path, base_extraction_path, oasis_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moderately demented: 28 images\n",
      "mildly demented: 70 images\n",
      "severely demented: 2 images\n",
      "nondemented: 134 images\n"
     ]
    }
   ],
   "source": [
    "# Uniqueify the identifiers. \n",
    "\n",
    "import os\n",
    "\n",
    "def count_images(data_dir):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            count = len([name for name in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, name))])\n",
    "            class_counts[class_name] = count\n",
    "    return class_counts\n",
    "\n",
    "# Example usage\n",
    "data_dir = './data/'  # Replace with your actual data directory path\n",
    "counts = count_images(data_dir)\n",
    "for class_name, count in counts.items():\n",
    "    print(f\"{class_name}: {count} images\")\n",
    "\n",
    "# This will output the count of images in each subdirectory of 'notebooks/data'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62ef8ab7a50fde946441678ef251eba85852e5ab7813d8beba261a97f4cf7750"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit ('nlp-m': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
